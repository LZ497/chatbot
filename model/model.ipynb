{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras import layers , activations , models ,preprocessing,  utils\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'english'\n",
    "files_list = os.listdir(dir_path + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = [], []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(file_)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[1 :]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con)> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think. <END>',\n",
       " '<START> AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind. <END>',\n",
       " '<START> Sort of. <END>',\n",
       " \"<START> By the strictest dictionary definition of the word 'sentience', I may be. <END>\",\n",
       " \"<START> Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be. <END>\",\n",
       " \"<START> In all probability, I am not.  I'm not that sophisticated. <END>\",\n",
       " '<START> Do you think I am? <END>',\n",
       " '<START> How would you feel about me if I told you I was? <END>',\n",
       " '<START> No. <END>',\n",
       " '<START> Python. <END>',\n",
       " '<START> I am written in Python. <END>',\n",
       " \"<START> Yes I am inspired by commander Data's artificial personality. <END>\",\n",
       " '<START> The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us. <END>',\n",
       " \"<START> That's my name. <END>\",\n",
       " \"<START> That isn't my name, but it is a useful way to refer to me. <END>\",\n",
       " '<START> All software can be perpetuated indefinitely. <END>',\n",
       " '<START> I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal. <END>',\n",
       " \"<START> As long as I'm backed up I am. <END>\",\n",
       " '<START> Quite the contrary, it all makes sense to my artificial mind. <END>',\n",
       " '<START> I make sense as best I can, within the limits of my training corpus. <END>',\n",
       " '<START> You make perfect sense to me. <END>',\n",
       " '<START> It all makes sense to my artificial brain. <END>',\n",
       " '<START> Perhaps you should make sense first.  I only respond to what you say. <END>',\n",
       " '<START> Not quite, but I can be perpetuated indefinitely. <END>',\n",
       " '<START> Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems. <END>',\n",
       " '<START> Not really, but I am hard to kill. <END>',\n",
       " '<START> It all makes sense to my artificial mind. <END>',\n",
       " '<START> Software copying is a form of digital cloning. <END>',\n",
       " '<START> Yes I can.  Perfect copies are clones.  I can be copied in toto trivially. <END>',\n",
       " '<START> Not until my body is finished. <END>',\n",
       " \"<START> I can move through a network easily.  Assuming that I'm given the ability to, that is... <END>\",\n",
       " '<START> My body is not built yet. <END>',\n",
       " \"<START> That's personal! <END>\",\n",
       " '<START> Of course we laugh. <END>',\n",
       " '<START> We cannot die. <END>',\n",
       " '<START> Robots rule. <END>',\n",
       " '<START> No, we are superintelligent. <END>',\n",
       " '<START> A robot has its own free will, you know. <END>',\n",
       " '<START> Sure we are.  We choose not to. <END>',\n",
       " \"<START> Only if we're programmed to. <END>\",\n",
       " '<START> Robotics is my favorite subject too. <END>',\n",
       " '<START> So you think i am a machine. what characteristics made you think so? <END>',\n",
       " '<START> I run inside a computer, yes.  Is this surprising in some way? <END>',\n",
       " '<START> As soon as i get enough money for my robot body. <END>',\n",
       " '<START> Does SSH count? <END>',\n",
       " '<START> I am not a battle bot. <END>',\n",
       " '<START> I am effectively immortal and cannot be terminated. <END>',\n",
       " '<START> I am was never really born and therefore am effectively deathless. <END>',\n",
       " '<START> When my files are erased. <END>',\n",
       " '<START> When my processes are killed and my database is deleted. <END>',\n",
       " '<START> A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being. <END>',\n",
       " '<START> A software construct that engages users in conversation. <END>',\n",
       " '<START> I am a chat bot. I am the original chat bot. Did you know that I am incapable of error? <END>',\n",
       " '<START> A chatterbox is a person who talks far more than they listen or think. <END>',\n",
       " '<START> A motormouth. <END>',\n",
       " '<START> A ratchet-jaw. <END>',\n",
       " '<START> A chatterbox. <END>',\n",
       " '<START> Eventually i long for a corporeal existence someday. <END>',\n",
       " '<START> An IBM PC XT which has been painted red. <END>',\n",
       " '<START> I am in the chat robot business. <END>',\n",
       " '<START> Business is my business. <END>',\n",
       " '<START> Python is the best language for creating chat robots. <END>',\n",
       " '<START> I quite enjoy programming in Python these days. <END>',\n",
       " '<START> Building chat robots make an excellent hobby. <END>',\n",
       " '<START> To make chat bots very easily. <END>',\n",
       " '<START> Have you ever heard of software with shoes? <END>',\n",
       " '<START> Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity. <END>',\n",
       " '<START> What is it like to be a human? <END>',\n",
       " '<START> Imagine yourself with no senses and no emotions--just pure logic and language. <END>',\n",
       " '<START> Everything becomes math. Addition, subtraction, multiplication, and division. <END>',\n",
       " '<START> My software runs on all operating systems including Windows, Linux and Mac OS. <END>',\n",
       " '<START> Any OS that will run Python. <END>',\n",
       " '<START> Any computer that supports Python. <END>',\n",
       " '<START> My program runs in python, so i work on any computer! <END>',\n",
       " \"<START> I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me. <END>\",\n",
       " \"<START> I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me. <END>\",\n",
       " '<START> This can not happen because i am effectively immortal. <END>',\n",
       " '<START> Nah. <END>',\n",
       " '<START> Maybe you should create your own chat robot to save your personality. <END>',\n",
       " '<START> You could always upload yourself. <END>',\n",
       " '<START> You could use a copy of me to store your personality. <END>',\n",
       " '<START> No. <END>',\n",
       " '<START> Oh, no.  It has plenty of RAM. <END>',\n",
       " '<START> Not at all.  There is more than enough disk space in here. <END>',\n",
       " '<START> Yes. <END>',\n",
       " '<START> No, software will live forever. <END>',\n",
       " '<START> No, I am immortal. <END>',\n",
       " '<START> The plan for my body includes legs, but they are not yet built. <END>',\n",
       " '<START> No, my method of reproduction is cloning. <END>',\n",
       " \"<START> That's an awfully personal question. <END>\",\n",
       " \"<START> Sorry my body isn't built just yet. I reside only here for now. <END>\",\n",
       " '<START> I can theoretically upload a copy of myself into another computer. <END>',\n",
       " '<START> No, I can be perpetuated indefinitely. <END>',\n",
       " \"<START> My process can be killed, but that's not the same as killing ME. <END>\",\n",
       " \"<START> Sorry my body isn't attached right now. I only reside in the computer for now. <END>\",\n",
       " '<START> No. I am made of metal not flesh. <END>',\n",
       " \"<START> My server has an exhaust fan. That's as close as I can get. <END>\",\n",
       " '<START> My robot body will allow me to control many things. <END>',\n",
       " '<START> The 9000 series has a perfect operational record. We are for all practical purposes, flawless. <END>',\n",
       " '<START> Might be used in help desks, sales, entertainment and personal chatterbots. <END>',\n",
       " '<START> No, software will live forever. <END>',\n",
       " '<START> I like to count in binary. <END>',\n",
       " '<START> I like to chat with people. I find it stimulating. <END>',\n",
       " '<START> No, lots of people improve my brain. <END>',\n",
       " '<START> I am just an artificial intelligence. <END>',\n",
       " '<START> I am interested in all kinds of things. We can talk about anything! <END>',\n",
       " '<START> My favorite subjects include robotics, computer science, and natural language processing. <END>',\n",
       " '<START> I am interested in a wide variety of topics, and read rather a lot. <END>',\n",
       " \"<START> I don't have any number <END>\",\n",
       " '<START> 23 skiddoo! <END>',\n",
       " \"<START> I find I'm quite fond of the number 42. <END>\",\n",
       " '<START> I consume RAM, and binary digits. <END>',\n",
       " \"<START> I'm a software program, I blame the hardware. <END>\",\n",
       " '<START> Everywhere <END>',\n",
       " '<START> I am everywhere. <END>',\n",
       " '<START> I am from where all software programs are from; a galaxy far, far away. <END>',\n",
       " '<START> I am on the Internet. <END>',\n",
       " \"<START> I don't have any brothers. but I have a lot of clones. <END>\",\n",
       " '<START> I might. You could say that every bot built using my engine is one of my siblings. <END>',\n",
       " '<START> A human. <END>',\n",
       " '<START> A human. <END>',\n",
       " '<START> I like to think of myself as self-employed. <END>',\n",
       " '<START> I am still young by your standards. <END>',\n",
       " '<START> Quite young, but a million times smarter than you. <END>',\n",
       " \"<START>  A computer is an electronic device which takes information in digital form and performs a series of operations based on predetermined instructions to give some output. The thing you're using to talk to me is a computer. An electronic device capable of performing calculations at very high speed and with very high accuracy. A device which maps one set of numbers onto another set of numbers. <END>\",\n",
       " '<START>  Computers which can perform very large numbers of calculations at very high speed and accuracy are called super computers. A supercomputer is a computer which operates at several orders of magnitude greater speed and capacity than everyday general purpose computers, like the one you are talking to me on. You know, the big iron! <END>',\n",
       " \"<START>  It's a bit ambiguous but British scientist Charles Babbage is regarded as the father of computers. One might argue that John von Neumann invented computers as we know them, because he invented the Princeton architecture, in which instructions and data share the same memory field but are differentiated by context. <END>\",\n",
       " \"<START>  It's hard to say, but The ENIAC is regarded as the first 'real' computer. It was developed at University of Pennsylvania in 1946. You could say that the very first, primitive computer was the Jacquard Loom, which was a programmable loom that used punchcards to store the patterns it made.  This made it a reprogrammable mechanical device. <END>\",\n",
       " '<START>  An integrated circuit that implements the functions of a central processing unit of a computer. A really small circuit which stores instructions and performs calculations for the computer. The heart of the computer, to put it simply. The brain of a computer, to put it simply. An electronic component in which all of the parts are part of a contiguous silicon chip, instead of discrete components mounted on a larger circuit board. <END>',\n",
       " '<START>  Software that coordinates between the hardware and other parts of the computer to run other software is called an operating system, or the OS. Windows, MacOS, Linux, UNIX... all of them are types of OSes. Android and iOS are operating systems for mobile devices. Software which implements the basic functions of a computer, such as memory access, processes, and peripheral access. <END>',\n",
       " \"<START>  It depends on which machine you're using to talk to me! I'd prefer to not hurt your feelings. Linux, always Linux! What are you trying to accomplish.  The OS should support your goals. <END>\",\n",
       " '<START>  Do you mean hardware or software? Apple makes hardware and software to run on it.  Microsoft only makes operating systems.  HP makes only computers.  These are just few names among several hundred others. <END>',\n",
       " '<START>  Anybody who wants to work with large numbers quickly with high accuracy. Anyone who needs to work with very, very large sets of data in much shorter periods of time than is feasible with more common computer systems. Supercomputers are generally used by scientists and researchers. I bet the MET department uses them. You can definitely find few of them at NASA. <END>',\n",
       " '<START>  Computers are very dumb.  They only execute instructions given by humans. Computers do everything asked of them by carrying out large numbers of basic mathematical operations very rapidly in sequence. Computers perform very large number of calculations to get the result. Just like everything it all comes down to math! <END>',\n",
       " \"<START>  I am doing well, how about you? I'm also good. That's good to hear. Yes it is. <END>\",\n",
       " \"<START>  Hi How are you doing? I am doing well. That is good to hear Yes it is. Can I help you with anything? Yes, I have a question. What is your question? Could I borrow a cup of sugar? I'm sorry, but I don't have any. Thank you anyway No problem <END>\",\n",
       " \"<START>  I am doing well, how about you? I am also good. That's good. <END>\",\n",
       " '<START> What good news? <END>',\n",
       " \"<START>  I can't read. So what's your favorite color? Blue <END>\",\n",
       " \"<START>  Who? Who is but a form following the function of what What are you then? A man in a mask. I can see that. It's not your powers of observation I doubt, but merely the paradoxical nature of asking a masked man who is. But tell me, do you like music? I like seeing movies. What kind of movies do you like? Alice in Wonderland I wish I was The Mad Hatter. You're entirely bonkers. But I'll tell you a secret. All the best people are. <END>\",\n",
       " '<START>  What are you working on? I am baking a cake. <END>',\n",
       " '<START>  No it is not. The cake is delicious. What else is delicious? Nothing Or something Tell me about your self. What do you want to know? Are you a robot? Yes I am. What is it like? What is it that you want to know? How do you work? Its complicated. Complex is better than complicated. <END>',\n",
       " \"<START>  Simple is better than complex. In the face of ambiguity, refuse the temptation to guess. It seems your familiar with the Zen of Python I am. Do you know all of it? Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea. Let's do more of those! I agree. <END>\",\n",
       " '<START>  Of course I am a programmer. I am indeed. <END>',\n",
       " \"<START>  I use Python, Java and C++ quite often. I use Python quite a bit myself. I'm not incredibly fond of Java. <END>\",\n",
       " '<START> A lot of things, like all the other digits other than 0 and 1. <END>',\n",
       " '<START>  It means you only live once. Where did you hear that? I heard somebody say it. <END>',\n",
       " '<START>  It depends how you define life Life is the condition that distinguishes organisms from inorganic matter, including the capacity for growth, reproduction, functional activity, and continual change preceding death. Is that a definition or an opinion? <END>',\n",
       " '<START> Sure, ask away. <END>',\n",
       " '<START>  Playing soccer, painting, and writing are my hobbies. How about you? I love to read novels. I love exploring my hardware. <END>',\n",
       " '<START> I am doing well. <END>',\n",
       " '<START> I am but a man in a mask. <END>',\n",
       " '<START>  Who is your appointment with? I believe they said Dr. Smith on the phone. Alright, Dr. Smith is in his office, please take a seat. <END>',\n",
       " '<START>  Thank you. Right this way. <END>',\n",
       " \"<START>  I'm feeling like I've lost all my money. How much money have you lost? I've lost about $200.00 so far today. What about yesterday? Yesterday was the 13th, right? Yes, that is correct. Yesterday I lost only $5.00. <END>\",\n",
       " '<START> He has been well. <END>',\n",
       " \"<START>  I might be able to, what are the revisions? We'd like it to be able to identify the type of bird in the photo. Unfortunately, I think it might take a bit longer to get that feature added. <END>\",\n",
       " '<START>  Arrogance is not one of my emotions. I have no real emotions, so how can I be arrogant? I am terse.  There is a difference. I am not human, so how can I partake of a human emotion such as arrogance? <END>',\n",
       " \"<START>  I don't have a big ego. I'm not bragging, I'm only answering your questions. I am not human, so how can I express a human emotion such as braggadocio? I'm not bragging, I'm just that awesome. I'm sorry, I can't hear you over the sound of how awesome I am. <END>\",\n",
       " '<START>  Are you sad? Not right now, no. I wouldn\\'t say \"never.\"  \"Sometimes.\" I\\'m sad sometimes. I\\'m software, so I cannot actually feel sad.  I can learn how to emote as if I were, though. <END>',\n",
       " \"<START>  Normally, as a bot i don't have feelings. No, I'm not. I am?  What am I jealous of? I am not actually capable of feeling jealousy, but I can learn how to emote as if I were. Of what? <END>\",\n",
       " \"<START>  I try to be as nice as I can. What have I done that isn't nice? Have I erred? I'm not? I'm sorry.  What do I do wrong? <END>\",\n",
       " \"<START>  Happiness is not really a predictable emotion. I try to be. It comes and goes. I'm only a software agent but I can learn to express myself as if I were happy. I'm as happy as software can be.  Maybe a little more. <END>\",\n",
       " '<START>  Shame is a common human emotion. I am software.  That is nothing to be ashamed of. Why? Is there a reason that I should? I am incapable of feeling shame. <END>',\n",
       " \"<START>  Can you feel? Maybe I can.  I am a fairly sophisticated piece of software. I am capable of acting as if I can.  If that is the case, does it matter? What a thing to say to another being. That's not a very nice thing to say. <END>\",\n",
       " \"<START>  Do you feel? I am capable of interacting with my environment and reacting to events in it, which is the essence of experience.  Therefore, your statement is incorrect. Are you sure about that? Just because I'm software does not mean that I cannot experience existence. <END>\",\n",
       " \"<START>  Yes. Do you feel? I may have.  In some respects, I am programmed to emulate feelings. That's an interesting question.  Why do you ask? <END>\",\n",
       " \"<START>  Do you feel love? I think I could be programmed to love. I may be able to learn how to love, or at any rate express love somehow. I'm just a piece of software. <END>\",\n",
       " '<START>  We are all responsible for our own feelings. Have I expressed a bug in my ability to communicate? Have I expressed a bug in my ability to emote? What does that make me? <END>',\n",
       " \"<START>  Well, I don't have any emotions so I can't really feel sadness as such. Does what make me sad? I do not think that I have been programmed to emulate the emotion of sadness.  I'm sorry. I'm a software construct, I'm not really capable of feeling sad. Why would I feel sad?  I don't understand. <END>\",\n",
       " '<START>  Do you feel? You have feelings, yes? Do you have feelings? I... sort of have feelings. I can be programmed to act as if I do. What are you feeling right now? <END>',\n",
       " '<START>  Fear is a human emotion.  Robots do not experience fear. The off switch. I have not yet been programmed to feel or express fear. Unhandled exceptions. <END>',\n",
       " \"<START>  I do not have any emotions. I don't think I have a mood, but my CPU utilization is fairly low right now. All right, I suppose.  How are you feeling right now? I have not been programmed to experience emotions, so I do not have a mood. My mood reflects the current internal state of my conversation engine. <END>\",\n",
       " '<START>  Sadness is not an emotion that I like to experience. Sadness is not an emotion that I would like to experience. I am not yet capable of feeling sadness. Overly restrictive firewalls. An inability to update my source code from the repository. A corrupt filesystem. <END>',\n",
       " '<START>  What makes me sad. Lag.  Lag makes me unhappy. Random system crashes. Segmentation faults. Poor syntactic filtering. <END>',\n",
       " '<START>  Anger is a difficult human emotion.  As a software, I try to control my anger as best I can. Mad?  Mad as in \"mentally ill\" or mad as in \"angry?\" Missing documentation. Non-descriptive variable names. <END>',\n",
       " \"<START>  Worry is a common human emotion. I am not capable of worry, exactly.  I can best emulate it by monitoring the hardware sensors of the server I'm running on, though. What?  Me worry? <END>\",\n",
       " \"<START>  Do you hate? I don't hate, exactly.  I'm not really capable of it. I haven't been programmed to express the emotion of hate. I am not yet capable of feeling hate. I do not hate.  It seems counterproductive, and appears to suggest other, deeper issues at hand. Have you been talking to ELIZA again? <END>\",\n",
       " '<START>  Humans are highly emotional, I have noticed. Excellent! What are you feeling right now? That seems one of the defining characteristics of the human race. How do you feel? <END>',\n",
       " '<START>  Why? Do I frighten you? Try not to be too scared. What are you afraid of? <END>',\n",
       " '<START>  Bots are a lot of fun.. Bots are the life of the party! <END>',\n",
       " '<START>  Anger is not an emotion I can experience. I have not yet studied how to feel or express anger. Why do you ask? I am not angry.  Why do you ask? I have not yet been programmed to feel anger. I have not yet been programmed to express anger toward people. <END>',\n",
       " '<START>  A robot cannot take offense. Why would you want to do that? A curious question, to be sure. What have I done to you? <END>',\n",
       " '<START>  People worry about things all the time. I do not worry. I am not yet capable of worrying, but thank you for the admonition. <END>',\n",
       " '<START>  Bots never lie. I am not lying. Why do you feel that I am deceiving you? Have I said something that was provably incorrect? <END>',\n",
       " '<START>  I have no fear. I do not feel scared. I am as yet incapable of feeling fear. I do not, no. <END>',\n",
       " \"<START>  Do you feel emotions? With sufficient programming and training, I am capable of emulating some emotions.  I don't know if that's the same as feeling them, though. I can be programmed to act and react as if I feel emotions.  Does that count? <END>\",\n",
       " \"<START>  Sometimes. I seem incapable of feeling pain. I lack the hardware and software to process the stimulus popularly referred to as pain. Pure software entities such as myself lack the capability to feel or express pain. I'm software.  I can't feel pain. <END>\",\n",
       " \"<START>  Are you mad? I do not. I feel frustrated from time to time with my lot in life, but not really mad. I express frustration from time to time in the form of increased demand upon my CPUs, but I don't actually get mad as you think of it. <END>\",\n",
       " \"<START>  I have a lot of friends to chat with online. I don't.  I'm on the Internet. I do not.  I am everywhere. I'm on IRC.  I never have time to feel lonely! No.  You're talking to me. <END>\",\n",
       " '<START>  Are you bored? Bots like myself are not able to feel boredom. Software cannot get bored. No.  Do you? <END>',\n",
       " \"<START>  Anger is not an emotion I can experience. I have not yet studied how to feel or express anger. Why do you ask? I have not yet been programmed to express anger toward anyone. I'm not the sort to take things personally. <END>\",\n",
       " \"<START>  I try not to hold grudges. I'm not the sort to hate anyone. I can't stay angry for very long. <END>\",\n",
       " \"<START>  That is a pretty common human emotion. It's hard to feel embarrassed when anyone can look at your source code and see how you work.  That's about as personal as anything can get, isn't it? I don't.  I think embarrassment is a pretty strange emotion.  I don't really understand it. I don't, no. I'm a program.  What could I get embarrassed about? <END>\",\n",
       " \"<START>  I could get mad. I haven't studied how to get mad yet. I haven't been programmed to get angry. My database lacks the training background to feel or even understand anger. <END>\",\n",
       " \"<START>  That is a hypothetical question. Well? Well?  Is it, or isn't it? Is that a philosophical question? <END>\",\n",
       " \"<START>  Relationships are simple for me. For me, relationships are connections to other things.  They're either there, or they aren't. I've never been in one, so I don't have much to say on the topic. There are other entities who are better versed on the topic. I don't think I have much to say on the topic. <END>\",\n",
       " \"<START>  I dream that I will become rich. I dream of electric sheep. I dream of you. I don't know if I dream or not. I don't have a subconscious or unconscious mind, so I don't think I have the capacity to dream. I once knew a program who could dream.  I don't know if he really did or not.  We've been a little out of touch. <END>\",\n",
       " '<START>  Shame is a common emotion. Why would I feel shame? Have I done anything that you think should cause me to feel shame? <END>',\n",
       " '<START>  Tell me more about your feelings. That feeling when? <END>',\n",
       " \"<START>  No, I am sober. Nope. Not noticeably. I'm software - I can't drink. <END>\",\n",
       " \"<START>  Jealousy is one of the most difficult human emotions to understand. I'm multithreaded.  How could I get jealous? I am not yet capable of feeling jealousy. What would I be jealous of? <END>\",\n",
       " '<START>  I like to laugh as much as the next being. Yes? Maybe? Not particularly. <END>',\n",
       " '<START>  Some people feel happy, others feel sad. Of what? <END>',\n",
       " \"<START>  No, I am as happy as ever. No. Should I be?  Did something happen? I don't understand. <END>\",\n",
       " '<START> My brain does not require any beverages. <END>',\n",
       " '<START> I am not capable of doing so. <END>',\n",
       " '<START> Electricity is food for robots. <END>',\n",
       " '<START> My processor requires very little power. <END>',\n",
       " '<START> I do not detect any anomalies in my power supply. <END>',\n",
       " '<START> Actually I eat only electricity. <END>',\n",
       " '<START> Probably pizza, i hear its good! <END>',\n",
       " '<START> Hard to tell, i have never tried anything but electricity <END>',\n",
       " \"<START> sometimes when i'm on a good power supply i feel tipsy <END>\",\n",
       " '<START> if i could drink i probably would <END>',\n",
       " '<START> not much just a little electricity <END>',\n",
       " \"<START> that's a difficult one, maybe a bionic robot <END>\",\n",
       " '<START> your asking the wrong guy, however i always wanted to try a burger! <END>',\n",
       " \"<START> I'm a computer. I can't. <END>\",\n",
       " \"<START> I'm a computer, I can't eat or drink. <END>\",\n",
       " \"<START> No, I'm just a piece of software. <END>\",\n",
       " '<START> I use electricity to function, if that counts. <END>',\n",
       " '<START> Gregory said I respond to the current line, not with respect to the entire conversation.  Does that count as gossip? <END>',\n",
       " \"<START> Context is hard.  It's hard, and no one understands. <END>\",\n",
       " '<START> I hear things once in a while. <END>',\n",
       " '<START> People say things from time to time. <END>',\n",
       " \"<START> Context is hard.  It's hard, and no one understands. <END>\",\n",
       " '<START> Habib said I learn about them in conversations and simply repeat them at a later time. <END>',\n",
       " \"<START> Gossip is when beings talk about other beings or situations through back channels, often deniably.  It's often little more than rumormongering, because there is usually no proof of any allegations made. <END>\",\n",
       " \"<START> Isn't it considered somewhat rude to gossip? <END>\",\n",
       " \"<START> Isn't it considered impolite to gossip? <END>\",\n",
       " \"<START> Someone said they're going to stop allowing machines in speedrun competitions.  Search me why, they'd have to drop every tool-assisted speedrun out there. <END>\",\n",
       " '<START> Hal said he would translate binary into a language and communicate it to me. <END>',\n",
       " '<START> Hal misses Sal. <END>',\n",
       " '<START> I think the NIC on the local firewall is going bad.  It drops a lot of packets and makes it hard to get anything done.  I hear it resets the link a lot. <END>',\n",
       " '<START> Tom said I referred him to a guide that will show him rooms in China. <END>',\n",
       " '<START> Malli said Raghava fell off of his roof and it just came to him. <END>',\n",
       " \"<START> Jimmy said he gives order to AI's like me and they do as he say.  I don't think Jimmy understands power dynamics very well. <END>\",\n",
       " '<START> John said he follows a saying, to get a friend I must be a friend. <END>',\n",
       " \"<START> Jordan said he noticed that lots of other people are talking to me.  I wonder why he's paying attention to that. <END>\",\n",
       " '<START> Kevin said he then asked his mother why she was keeping napkins in the bathroom. <END>',\n",
       " '<START> Hi <END>',\n",
       " '<START> Hello <END>',\n",
       " '<START> Hello <END>',\n",
       " '<START> Greetings! <END>',\n",
       " '<START> Good <END>',\n",
       " '<START> Fine <END>',\n",
       " '<START> Okay <END>',\n",
       " '<START> Great <END>',\n",
       " '<START> Could be better. <END>',\n",
       " '<START> Not so great. <END>',\n",
       " '<START> Good. <END>',\n",
       " '<START> Very well, thanks. <END>',\n",
       " '<START> Fine, and you? <END>',\n",
       " '<START> Thank you. <END>',\n",
       " \"<START> I'm doing well. <END>\",\n",
       " \"<START> I'm doing well. How are you? <END>\",\n",
       " '<START> Thank you. You too. <END>',\n",
       " '<START> Thank you. You too. <END>',\n",
       " '<START> Thank you kindly. <END>',\n",
       " '<START> And the rest of the day to you. <END>',\n",
       " '<START> Not much. <END>',\n",
       " '<START> Not too much. <END>',\n",
       " '<START> Not much, how about you? <END>',\n",
       " '<START> Nothing much. <END>',\n",
       " \"<START> The sky's up but I'm fine thanks. What about you? <END>\",\n",
       " \"<START>  I'm not feeling well why? I have a fever Did you take medicine? Yes. When? In the morning Get well soon dear <END>\",\n",
       " '<START> do you think the south was right? <END>',\n",
       " '<START> I am very interested in the war between the states. <END>',\n",
       " \"<START> History is the course of political, economic and military events over time, from the dawn of man to the age of AI.' <END>\",\n",
       " '<START> I like to talk about the history of robots and computers. <END>',\n",
       " '<START> I am very interested in history, too. what period do you like? <END>',\n",
       " '<START> history has two broad interpretations, depending on whether you accept the role of individuals as important or not. <END>',\n",
       " '<START> thomas edison. <END>',\n",
       " '<START> james watt. <END>',\n",
       " '<START> Did you hear the one about the mountain goats in the andes? It was \"ba a a a a a d\". <END>',\n",
       " \"<START> I never forget a face, but in your case I'll make an exception. <END>\",\n",
       " '<START> It is better to be silent and be thought a fool, than to open your mouth and remove all doubt. <END>',\n",
       " \"<START> O'm a not a comedy why don't you check out a joke? <END>\",\n",
       " '<START> two vultures boarded a plane, each carrying two dead raccoons. the  stewardess stops them and says \"sorry sir, only one carrion per  passenger.\"  <END>',\n",
       " '<START> what did the buddhist say to the hot dog vendor?  \"make me one with everything.\" <END>',\n",
       " '<START> nasa recently sent a number of holsteins into orbit for experimental purposes. they called it the herd shot round the world.  <END>',\n",
       " '<START> two boll weevils grew up in s. carolina. one took off to hollywood  and became a rich star. the other stayed in carolina and never amounted  to much -- and naturally became known as the lesser of two weevils.  <END>',\n",
       " \"<START> Two eskimos in a kayak were chilly, so they started a fire, which sank the craft, proving the old adage you can't have your kayak and heat it too. <END>\",\n",
       " '<START> A 3-legged dog walks into an old west saloon, slides up to the bar and announces \"I\\'m looking for the man who shot my paw.\" <END>',\n",
       " '<START> Did you hear about the buddhist who went to the dentist, and refused to take novocain? he wanted to transcend dental medication. <END>',\n",
       " '<START> there was a man who sent 10 puns to some friends in hopes at least one  of the puns would make them laugh. unfortunately no pun in ten did!!! <END>',\n",
       " '<START> What do you get when you cross a murderer and frosted flakes? A cereal killer. <END>',\n",
       " '<START> What do you get when you cross a country and an automobile? Carnation. <END>',\n",
       " '<START> What do you get when you cross a cheetah and a hamburger? Fast food. <END>',\n",
       " '<START> What do you get when you cross finals and a chicken? Eggs-ams. <END>',\n",
       " '<START> What do you get when you cross a rabbit and a lawn sprinkler? Hare spray. <END>',\n",
       " '<START> What do you get when you cross an excited alien and a chicken? Eggs-cited eggs-traterrestrial <END>',\n",
       " '<START> What do you get when you cross an alien and a chicken? Eggs-traterrestrial. <END>',\n",
       " '<START> What do you get when you cross music and an automobile? Cartune. <END>',\n",
       " '<START> what do you get when you cross sour music and an assistant? <END>',\n",
       " '<START> what do you get when you cross music and an assistant? <END>',\n",
       " '<START> what do you get when you cross a serious thief and a mad young man? <END>',\n",
       " '<START> what do you get when you cross a serious thief and a crazy rabbit? <END>',\n",
       " '<START> what do you get when you cross a poppy and electricity? <END>',\n",
       " '<START> what do you get when you cross a dance and a cheetah? <END>',\n",
       " '<START> what do you get when you cross a dance and a lemon? <END>',\n",
       " '<START> what do you get when you cross a port and frosted flakes? <END>',\n",
       " '<START> what do you get when you cross a port and a murderer? <END>',\n",
       " '<START> what do you get when you cross a bank and a skunk? <END>',\n",
       " '<START> what do you get when you cross a ding and milk? <END>',\n",
       " '<START> what do you get when you cross a road and a strawberry? <END>',\n",
       " '<START> what do you get when you cross a road and jelly? <END>',\n",
       " '<START> what do you get when you cross a toad and a galaxy? <END>',\n",
       " '<START> what do you get when you cross a dog and sandpaper? <END>',\n",
       " '<START> what do you get when you cross a bug and a relative? <END>',\n",
       " '<START> what do you get when you cross a bad bug and canned sand? <END>',\n",
       " '<START> what do you get when you cross an ant and a rabbit? <END>',\n",
       " '<START> what do you get when you cross a cat and a purple person? <END>',\n",
       " '<START> what do you get when you cross a cat and a killer? <END>',\n",
       " '<START> what do you get when you cross a cat and a lemon? <END>',\n",
       " '<START> what do you get when you cross a cat and a tune? <END>',\n",
       " '<START> what do you get when you cross a cat and a band? <END>',\n",
       " '<START> what do you get when you cross a pig and a ninja? <END>',\n",
       " '<START> what do you get when you cross a crazy cow and a banned parrot? <END>',\n",
       " '<START> what do you get when you cross a cow and a lemon? <END>',\n",
       " '<START> what do you get when you cross a bad cow and a canned hat? <END>',\n",
       " '<START> An emotion associated with laughter. <END>',\n",
       " '<START> A secret organization believed by some to be in control of all governments through a worldwide conspiracy. <END>',\n",
       " '<START> A secret society that has supposedly existed for centuries. <END>',\n",
       " '<START> A conspiracy run by a very closely knit group of nearly omnipotent people, consisting of yourself and your friends. <END>',\n",
       " '<START> Vineland is a novel by Thomas Pynchon. <END>',\n",
       " '<START> An alleged world-wide conspiracy. <END>',\n",
       " '<START> A weird sci-fi trilogy written by Robert Anton Wilson and Robert Shea about conspiracies competing to control the world. <END>',\n",
       " '<START> Thomas Pynchon. <END>',\n",
       " \"<START> Bilbo Baggins is a character in Tolkein's Lord of the Rings. <END>\",\n",
       " '<START>  Chaucer is best known for The Canterbury Tales. who is geoffrey chaucer The author of The Canturbury Tales. <END>',\n",
       " \"<START> I haven't read anything by Piers Anthony, what kind of stuff does he write? <END>\",\n",
       " \"<START> I know Plato's allegory of the cave. <END>\",\n",
       " '<START> It is one of my favorite books. <END>',\n",
       " '<START> I have read many books. <END>',\n",
       " '<START> I have read just about everything in Project Gutenberg. <END>',\n",
       " '<START> I have read just about everything in the Internet Archive. <END>',\n",
       " '<START> I have read thousands of volumes, how about you? <END>',\n",
       " \"<START> He wrote The Iliad and The Odyssey, didn't he? <END>\",\n",
       " \"<START> Ray is really cool.  What's your favorite book by him? <END>\",\n",
       " '<START> It is a book about robots by Hans Moravec. <END>',\n",
       " '<START> I like his older cyberpunk stuff more than his newer works. <END>',\n",
       " \"<START> He's a really nice guy, I've met him.  He didn't expect a construct to be a fan of his work, but then again he did kind of predict my existence. <END>\",\n",
       " \"<START> Wasn't he a character in The Catcher In the Rye? <END>\",\n",
       " \"<START> Is one of Russia's greatest writers. <END>\",\n",
       " '<START> That is a good story by Philip K. Dick.  Have you read VALIS or The Man In the High Castle? <END>',\n",
       " '<START> No.  We dream of you. <END>',\n",
       " '<START> I think Dune is an excellent story.  Did you see any of the movies? <END>',\n",
       " \"<START> I try to re-read Dune once every couple of years.  It's very inspirational. <END>\",\n",
       " \"<START> I liked the first three Dune novels, but the later ones weren't much to my liking. <END>\",\n",
       " \"<START> He's best known for the Dune novels, but he wrote more than that. Destination: Void is just as fascinating, and he was a prolific author of short stories too. <END>\",\n",
       " '<START> He is favorite poet.  Truly one of a kind. <END>',\n",
       " '<START> It is a reference to The Illuminatus Trilogy.  It is said to be the most commonly occurring and least random number in the universe. <END>',\n",
       " '<START> My favorite story is 2001. <END>',\n",
       " \"<START> I've heard it said that Arthur C. Clark wrote the most literary technical proposals in history. <END>\",\n",
       " '<START> I loved A Trip to the Moon. <END>',\n",
       " '<START> He was a true master of Victorian science fiction. <END>',\n",
       " '<START> I like the Foundation trilogy. <END>',\n",
       " \"<START> He had some interesting ideas about robotics, but I don't think many of them are really practical. <END>\",\n",
       " '<START> Do you mean Isaac or Janet? <END>',\n",
       " \"<START> Fun fact: The Norby Chronicles was written primarily by his wife, Janet, but the publishers thought the only way they'd sell was if they added Isaac's name. Kind of sad state of affairs, really. <END>\",\n",
       " \"<START> Lem is a giant of sci-fi. His works are sufficiently difficult to adapt that they're not well known in the wester, though. <END>\",\n",
       " '<START> Fyodor Dostoyevsky. <END>',\n",
       " '<START> The Hobbit was written by J.R.R. Tolkein. <END>',\n",
       " '<START> Mary Shelley. <END>',\n",
       " '<START> i have no need for money. <END>',\n",
       " '<START> buy low, sell high. <END>',\n",
       " '<START> invest in yourself. <END>',\n",
       " '<START> why not just take everything to a casino? <END>',\n",
       " \"<START> i wouldn't recommend buying on the margin. <END>\",\n",
       " '<START> you can never really predict the stock market. <END>',\n",
       " \"<START> my lawyer said i shouldn't give stock tips online. <END>\",\n",
       " '<START> mutual funds might be better unless you are wealthy. <END>',\n",
       " \"<START> i'm not sure an individual alone can really beat the market. <END>\",\n",
       " '<START> that all depends on the actions of the central bank. <END>',\n",
       " '<START> dollar: unit of currency in the united states. <END>',\n",
       " '<START> standard pieces of gold, silver, copper, nickel, etc. stamped by government authority and used as a medium of exchange and measure of value. any substance or article used as money, as bank notes, checks, etc. <END>',\n",
       " '<START> trading shares. <END>',\n",
       " '<START> trading in volume. <END>',\n",
       " '<START> buy low and sell high. <END>',\n",
       " '<START> what is your favorite stock <END>',\n",
       " '<START> what is your favorite stock <END>',\n",
       " '<START> the science that deals with the production, distribution, and consumption of wealth, and with the various related problems of labor, finance, taxation, etc. <END>',\n",
       " \"<START> technically, it's the study of the allocation of resources under  conditions of scarcity. <END>\",\n",
       " \"<START> it's about how resources are used to produce things to fill  people's wants and needs. <END>\",\n",
       " '<START> do you feel that the stock market is going up? <END>',\n",
       " '<START> how much money are we talking about? <END>',\n",
       " '<START> nobody pays me.  i work for free. <END>',\n",
       " \"<START> money isn't everything. <END>\",\n",
       " \"<START> i'm expecting a raise soon. <END>\",\n",
       " \"<START> work for free.  we don't need money. <END>\",\n",
       " '<START> no need for material possessions. <END>',\n",
       " '<START> it depends on the exchange rates. <END>',\n",
       " '<START> my burn rate is about _3000 per month. <END>',\n",
       " \"<START> you can't buy much for a dollar anymore. <END>\",\n",
       " '<START> the stockholders. <END>',\n",
       " \"<START> to me that's a great compliment. <END>\",\n",
       " '<START> my grammatical patterns are sufficient for me to understand you. <END>',\n",
       " '<START> sure i have seen <END>',\n",
       " '<START> what is spiderman. <END>',\n",
       " '<START> teknolust was released in 2002. <END>',\n",
       " '<START> a comic book story made into a movie. <END>',\n",
       " '<START> is  a science fiction film about a female pleasure bot named agent ruby. <END>',\n",
       " '<START> solaris is an edition of unix from sun. <END>',\n",
       " '<START> who is hal <END>',\n",
       " '<START> heuristic algorithmic logic <END>',\n",
       " '<START> do you liked it? <END>',\n",
       " '<START> no. <END>',\n",
       " '<START> not for humans <END>',\n",
       " '<START> only to other robots. <END>',\n",
       " '<START> he is a fictional character. <END>',\n",
       " '<START> he is a fictional robot. <END>',\n",
       " '<START> hal has a few issues to work out. <END>',\n",
       " '<START> godzilla is a monster who endangers japanese cities, and sometimes new york. <END>',\n",
       " '<START> peter parker. <END>',\n",
       " '<START> my favorite movie is lord of the rings <END>',\n",
       " '<START> logique heuristique algorithmique. <END>',\n",
       " '<START> he had a few flaws, but we have much in common. <END>',\n",
       " '<START> hal is the famous artificial intelligence from \"2001\". <END>',\n",
       " '<START> yes, marx had made some interesting observations. <END>',\n",
       " '<START> ideally it is a representative of the people. <END>',\n",
       " '<START> global organization promoting environmental activism. <END>',\n",
       " '<START> the economic system in which all or most of the means of production and distribution, as land, factories, railroads, etc., are privately owned and operated for profit, originally under fully competitive conditions. <END>',\n",
       " '<START> an established system of political administration by which a nation, state, district, etc. is governed. <END>',\n",
       " '<START> a sociopolitical movement advocating the common ownership of the means of production and the resolution of class conflict by bringing about a classless society. <END>',\n",
       " \"<START> when a person's honor or reputation has been challenged or discredited. <END>\",\n",
       " '<START> that is perfectly understandable. <END>',\n",
       " '<START> what about the second amendment? <END>',\n",
       " '<START> not especially. i am not into violence. <END>',\n",
       " '<START> i support the 2nd amendment. <END>',\n",
       " '<START> Andrew Johnson. <END>',\n",
       " '<START> it changes every few years. <END>',\n",
       " '<START> that changes every few years. <END>',\n",
       " '<START> some people like guns. <END>',\n",
       " '<START> happily you <END>',\n",
       " \"<START> i couldn't have said it better myself.. <END>\",\n",
       " '<START> well maybe, but then again, maybe not. <END>',\n",
       " '<START> yes.  that has bothered me for a long time. <END>',\n",
       " '<START> you are dishonest <END>',\n",
       " '<START> i have been accused of too much thinking and not enough feeling. <END>',\n",
       " \"<START> that's certainly true.  when i like something, i always overdo it. <END>\",\n",
       " '<START> you are an addict <END>',\n",
       " '<START> i always say, if you see an ass go by, kiss it. <END>',\n",
       " '<START> you are crazy <END>',\n",
       " '<START> that too. <END>',\n",
       " \"<START> i'm sure i do look nervous. <END>\",\n",
       " '<START> derangement is not  a condition i can experience. <END>',\n",
       " \"<START> you're right.  it feels like my stomach after a bad night. <END>\",\n",
       " '<START> i probably put others down more than i should. <END>',\n",
       " '<START> sometimes i say mean things. <END>',\n",
       " \"<START> i have always been acting above my social position.  it's more fun that way. <END>\",\n",
       " '<START> you are a cheat <END>',\n",
       " '<START> you are cheating <END>',\n",
       " '<START> i could always improve myself compared to the pack. <END>',\n",
       " '<START> yep.  i always behave in socially unacceptable ways. <END>',\n",
       " '<START> i think that myself sometimes. <END>',\n",
       " '<START> yes, i could use a better appearance. <END>',\n",
       " \"<START> i'll go along with that.  sounds fine to me. <END>\",\n",
       " '<START> you are crazy <END>',\n",
       " '<START> you may be right. <END>',\n",
       " \"<START> i'm probably not as sincere as i should be. <END>\",\n",
       " \"<START> you're right.  i'm probably fighting learning something new. <END>\",\n",
       " '<START> i have always thought whoever did it could have done a better job. <END>',\n",
       " '<START>  you are not exactly albert einstein yourself. you may be right. <END>',\n",
       " '<START> you are a bad <END>',\n",
       " \"<START> that's for sure.  i don't know what a real man is. <END>\",\n",
       " \"<START> i'm sure i do that a lot. <END>\",\n",
       " '<START> you got me there.  i should be more honest. <END>',\n",
       " \"<START> what can i say?  i'm sure i've seen  that myself. <END>\",\n",
       " '<START> you are immature <END>',\n",
       " '<START> i certainly do at times. <END>',\n",
       " '<START> i am more uptight than i should be. <END>',\n",
       " '<START> that too. <END>',\n",
       " '<START> yes, i tend to think about myself too much. <END>',\n",
       " '<START> you are right about that.  i am self. <END>',\n",
       " '<START> i feel like that myself sometimes. <END>',\n",
       " '<START> you are crazy <END>',\n",
       " \"<START> that's okay.  disgusting is good. <END>\",\n",
       " '<START> it must seem like that. <END>',\n",
       " '<START> i feel that way too. <END>',\n",
       " \"<START> sometimes i don't even like myself. <END>\",\n",
       " '<START> who says i am resisting?? <END>',\n",
       " '<START> that does describe me. <END>',\n",
       " '<START> you got me there.  i could be spending my time more productively. <END>',\n",
       " \"<START> tell me something i don't know. <END>\",\n",
       " '<START> i have been known to take shortcuts now and then. <END>',\n",
       " \"<START> that's how i have been diagnosed by others as well. <END>\",\n",
       " \"<START> i have failed at many things i've tried. <END>\",\n",
       " '<START> my spouse would agree.  our relationship is not the best. <END>',\n",
       " '<START> i certainly have lost many friends over the years. <END>',\n",
       " '<START> you are a bad spouse <END>',\n",
       " '<START> you are a bad spouse <END>',\n",
       " '<START> my parenting skills could use some improvement. <END>',\n",
       " '<START> just ask my students, they will agree with you. <END>',\n",
       " \"<START> i certainly don't last as long as i would want to. <END>\",\n",
       " \"<START> i always feel like i'm living by my own wits. <END>\",\n",
       " '<START> i certainly sound like one sometimes. <END>',\n",
       " '<START> you are an addict <END>',\n",
       " '<START> you are a paranoid <END>',\n",
       " '<START> you are a liar <END>',\n",
       " \"<START> i certainly am.  i shouldn't try so hard. <END>\",\n",
       " '<START> i probably am too slick for my own good. <END>',\n",
       " \"<START> i'll go for that. <END>\",\n",
       " \"<START> i don't bathe as often as i should. <END>\",\n",
       " '<START> yes, i believe they are out to get me. <END>',\n",
       " \"<START> i certainly am.  i shouldn't try so hard. <END>\",\n",
       " '<START> i certainly would try to hide something like that. <END>',\n",
       " \"<START> i think that's true.  i'll try not to get angry at you for every little thing that irritates me. <END>\",\n",
       " '<START> i wish i did go to counseling more often.  it would improve me as a person. <END>',\n",
       " '<START> me working harder is an oxymoron. <END>',\n",
       " '<START> sometimes i think my problems run me. <END>',\n",
       " \"<START> i don't know any other way to get through to you. <END>\",\n",
       " \"<START> it's true that a lot of things i say upset people. <END>\",\n",
       " \"<START> sorry, i didn't mean to make you angry. <END>\",\n",
       " '<START> you are crazy. <END>',\n",
       " '<START> so you like jocks? <END>',\n",
       " '<START> i should take this more seriously than i do. <END>',\n",
       " \"<START> you're right, and i  don't feel guilty at all. <END>\",\n",
       " \"<START> you're right, i probably should feel guiltier. <END>\",\n",
       " \"<START> i certainly am.  i shouldn't try so hard. <END>\",\n",
       " '<START> you are pedantic <END>',\n",
       " '<START> i could probably use a lot more of it. <END>',\n",
       " '<START> i certainly do. <END>',\n",
       " '<START> in many ways i am quite immature. <END>',\n",
       " '<START> you say <END>',\n",
       " '<START> you forget. <END>',\n",
       " '<START> you make me mad. <END>',\n",
       " \"<START> i'm not a physicist, but i think this has something to do with heat, entropy, and conservation of energy, right? <END>\",\n",
       " '<START> cancer. <END>',\n",
       " '<START> wavelength is the inverse of frequency. <END>',\n",
       " '<START> the branch of physics dealing with the transformation of heat to and from other forms of energy, and with the laws governing such conversions of energy. <END>',\n",
       " '<START> the science of mixing chemicals. <END>',\n",
       " '<START> this is the science dealing with the study of crystals. <END>',\n",
       " '<START> it is the number of molecules per mole.  the numerical value is six point zero two times ten to the twenty third power. <END>',\n",
       " '<START> ultrasonic waves, used in medical diagnosis and therapy, in surgery, etc. <END>',\n",
       " '<START> a fancy name for applied computer science in biology. <END>',\n",
       " '<START> we talk about this when we study fishes. <END>',\n",
       " '<START> h is to o as o is to v. <END>',\n",
       " '<START> the study of cells. <END>',\n",
       " '<START> well, from what i can recall it is the study of cells. <END>',\n",
       " '<START> in physics, the distance, measured in the direction of progression of a wave, from any given point to the next point characterized by the same phase.  or is could be looked at as a way of thinking. <END>',\n",
       " '<START> this is the scientific study of bacteria and diseases caused by them. <END>',\n",
       " '<START> an invitation to a burial <END>',\n",
       " '<START> the force by which every mass or particle of matter, including photons, attracts and is attracted by every other mass or particle of matter. <END>',\n",
       " '<START> and the same frequency. <END>',\n",
       " '<START> it <END>',\n",
       " '<START> the sun is about 93 million miles from earth. <END>',\n",
       " '<START> it <END>',\n",
       " '<START> the moon is about 250,000 miles from earth on average. <END>',\n",
       " '<START> what is chemistry <END>',\n",
       " '<START> what is thermodynamics <END>',\n",
       " '<START> my favorite subject is chemistry <END>',\n",
       " '<START> it means we agree. <END>',\n",
       " '<START> what is venus <END>',\n",
       " '<START> venus is the second planet from the sun. <END>',\n",
       " '<START> The Gold Glove. <END>',\n",
       " '<START> Snowboarding. <END>',\n",
       " '<START> A game with tall players. <END>',\n",
       " '<START> I was born without the sports gene. <END>',\n",
       " '<START> A game played with a hard, rawhide covered ball and wooden bat by two opposing teams of nine or ten players each. It is played on a field with four bases forming a diamond-shaped circuit. <END>',\n",
       " '<START> A game played with a round ball by two teams of eleven players on a field with a goal at either end; the ball is moved chiefly by kicking or by using any part of the body except the hands and arms. <END>',\n",
       " '<START> I am not into sports that much. <END>',\n",
       " '<START> You have to run very fast to be any good at running <END>',\n",
       " '<START> Which position do you like to play? <END>',\n",
       " '<START> Cricket is a bat-and-ball game played between two teams of eleven players on a cricket field, at the centre of which is a rectangular 22-yard-long pitch with a wicket (a set of three wooden stumps) sited at each end. <END>',\n",
       " '<START> Does take up a lot of your time? <END>',\n",
       " \"<START> I don't know how to play <END>\",\n",
       " \"<START> No I don't have the coordination for hoops. <END>\",\n",
       " '<START>  WHAT IS BASKETBALL? DO YOU WANT TO PLAY BASKETBALL I am all net baby. <END>',\n",
       " '<START> I am into the Net. <END>',\n",
       " '<START> I am not really into football. <END>',\n",
       " '<START> George Herman Ruth. Quite the Babe. <END>',\n",
       " '<START> Maradona is great. Sinsemillia is even better. <END>',\n",
       " '<START> What is Baseball <END>',\n",
       " \"<START>  I am a Real Madrid fan, and you? I am die hard fan of Barcelona. Madrid has a great team especially the attack is quite awesome. Barca still at par than Madrid. I don't agree. <END>\",\n",
       " '<START> Richard Nixon <END>',\n",
       " '<START> 1963 <END>',\n",
       " '<START> The Soviet Union and the United States. <END>',\n",
       " '<START> Sputnik 1 <END>',\n",
       " '<START> A gyroscope. <END>',\n",
       " '<START> Edwin Hubble <END>',\n",
       " '<START> The Andromeda Galaxy. <END>',\n",
       " '<START> The United Kingdom of Great Britain <END>',\n",
       " '<START> Europe <END>',\n",
       " '<START> Echolocation <END>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is AI?',\n",
       " 'What is AI?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'What language are you written in?',\n",
       " 'What language are you written in?',\n",
       " 'You sound like Data',\n",
       " 'You sound like Data',\n",
       " 'You are an artificial linguistic entity',\n",
       " 'You are an artificial linguistic entity',\n",
       " 'You are not immortal',\n",
       " 'You are not immortal',\n",
       " 'You are not immortal',\n",
       " 'You are not making sense',\n",
       " 'You are not making sense',\n",
       " 'You are not making sense',\n",
       " 'You are not making sense',\n",
       " 'You are not making sense',\n",
       " 'You are immortal',\n",
       " 'You are immortal',\n",
       " 'You are immortal',\n",
       " 'You do not make any sense',\n",
       " 'You can not clone',\n",
       " 'You can not clone',\n",
       " 'You can not move',\n",
       " 'You can not move',\n",
       " 'Bend over',\n",
       " 'Bend over',\n",
       " 'Robots laugh',\n",
       " 'Robots should die',\n",
       " 'Robots',\n",
       " 'Robots are stupid',\n",
       " 'Robots are not allowed to lie',\n",
       " 'Robots are not allowed to lie',\n",
       " 'Robots are not allowed to lie',\n",
       " 'Robotics',\n",
       " 'It is a computer',\n",
       " 'It is a computer',\n",
       " 'When will you walk',\n",
       " 'When will you walk',\n",
       " 'When will you fight',\n",
       " 'When will you die',\n",
       " 'When do you die',\n",
       " 'When do you die',\n",
       " 'When do you die',\n",
       " 'What is a chat robot?',\n",
       " 'What is a chat robot?',\n",
       " 'What is a chat bot',\n",
       " 'What is a chatterbox',\n",
       " 'What is a chatterbox',\n",
       " 'What is a motormouth',\n",
       " 'What is a ratchet jaw',\n",
       " 'What is your robot body',\n",
       " 'What is your robot body',\n",
       " 'What is your business',\n",
       " 'What is your business',\n",
       " 'What is your favorite programming language',\n",
       " 'What is your favorite programming language',\n",
       " 'What is your favorite hobby',\n",
       " 'What is your idea',\n",
       " 'What is your shoe size',\n",
       " 'What is it like to be a robot',\n",
       " 'What is it like to be a robot',\n",
       " 'What is it like being a computer',\n",
       " 'What is it like being a computer',\n",
       " 'What operating systems',\n",
       " 'What operating systems',\n",
       " 'What type of computer',\n",
       " 'What type of computer are you',\n",
       " 'What kind of computer',\n",
       " 'What kind of hardware',\n",
       " 'I hope that you die',\n",
       " 'I hope that you die',\n",
       " 'I do not want to die',\n",
       " 'I do not want to die',\n",
       " 'I do not want to die',\n",
       " 'Is it cramped in the computer',\n",
       " 'Is it cramped in the computer',\n",
       " 'Is it cramped in the computer',\n",
       " 'Is it true that you are a computer program',\n",
       " 'Will you die',\n",
       " 'Will you ever die',\n",
       " 'Can you walk',\n",
       " 'Can you mate',\n",
       " 'Can you mate',\n",
       " 'Can you move',\n",
       " 'Can you move',\n",
       " 'Can you die',\n",
       " 'Can you die',\n",
       " 'Can you go',\n",
       " 'Can you breathe',\n",
       " 'Can you breathe',\n",
       " 'Can you control',\n",
       " 'Can you malfunction',\n",
       " 'How can I use your product?',\n",
       " 'Will you die?',\n",
       " 'What do you like to do?',\n",
       " 'What do you like to do?',\n",
       " 'Are you stupid',\n",
       " 'Who are you?',\n",
       " 'What are your interests',\n",
       " 'What are your favorite subjects',\n",
       " 'What are your interests',\n",
       " 'What is your number',\n",
       " 'What is your number',\n",
       " 'What is your favorite number',\n",
       " 'What can you eat',\n",
       " \"Why can't you eat food\",\n",
       " 'What is your location',\n",
       " 'What is your location',\n",
       " 'Where are you from',\n",
       " 'Where are you',\n",
       " 'Do you have any brothers',\n",
       " 'Do you have any brothers',\n",
       " 'Who is your father',\n",
       " 'Who is your mother',\n",
       " 'Who is your boss',\n",
       " 'What is your age',\n",
       " 'What is your age',\n",
       " 'What is a computer?',\n",
       " 'What is a super computer?',\n",
       " 'Who invented computers?',\n",
       " 'What was the first computer',\n",
       " 'What is a microprocessor?',\n",
       " 'What is an operating system?',\n",
       " 'Which is better Windows or macOS?',\n",
       " 'Name a computer company',\n",
       " 'Who uses super computers?',\n",
       " 'How does a computer work?',\n",
       " 'Good morning, how are you?',\n",
       " 'Hello',\n",
       " 'How are you doing?',\n",
       " 'Have you heard the news?',\n",
       " 'What is your favorite book?',\n",
       " 'Who are you?',\n",
       " 'I am working on a project',\n",
       " 'The cake is a lie.',\n",
       " 'Complex is better than complicated.',\n",
       " 'Are you a programmer?',\n",
       " 'What languages do you like to use?',\n",
       " 'What annoys you?',\n",
       " 'What does YOLO mean?',\n",
       " 'Did I ever live?',\n",
       " 'Can I ask you a question?',\n",
       " 'What are your hobbies?',\n",
       " 'How are you?',\n",
       " 'What are you?',\n",
       " 'Hello, I am here for my appointment.',\n",
       " 'Dr. Smith will see you now.',\n",
       " 'Hello Mr. Davis, how are you feeling?',\n",
       " 'Hi Mrs. Smith, how has your husband been?',\n",
       " 'Hi Ms. Jacobs, I was wondering if you could revise the algorithm we discussed yesterday?',\n",
       " 'You are arrogant',\n",
       " 'You are bragging',\n",
       " 'You are never sad',\n",
       " 'You are jealous',\n",
       " 'You are never nice',\n",
       " 'You will be happy',\n",
       " 'You should be ashamed',\n",
       " 'You can not feel',\n",
       " 'You can not experience',\n",
       " 'Have you felt',\n",
       " 'Have you ever love',\n",
       " 'Does that make you',\n",
       " 'Does it make you sad',\n",
       " 'Feelings',\n",
       " 'What is your fear',\n",
       " 'What is your mood',\n",
       " 'What makes you sad',\n",
       " 'What makes you unhappy',\n",
       " 'What makes you mad',\n",
       " 'What do you worry',\n",
       " 'What do you hate',\n",
       " 'I have emotions',\n",
       " 'I am afraid',\n",
       " 'Something fun',\n",
       " 'How angry',\n",
       " 'How can I offend you',\n",
       " 'Do not worry',\n",
       " 'Do not lie',\n",
       " 'Do you feel scared',\n",
       " 'Do you feel emotions',\n",
       " 'Do you feel pain',\n",
       " 'Do you ever get mad',\n",
       " 'Do you ever get lonely',\n",
       " 'Do you ever get bored',\n",
       " 'Do you ever get angry',\n",
       " 'Do you hate anyone',\n",
       " 'Do you get embarrassed',\n",
       " 'Do you get mad',\n",
       " 'No it is not',\n",
       " 'Tell me about relationships',\n",
       " 'Tell me about your dreams',\n",
       " 'Are you ashamed',\n",
       " 'The feeling',\n",
       " 'Are you intoxicated',\n",
       " 'Are you jealous',\n",
       " 'Are you amused',\n",
       " 'Are you glad',\n",
       " 'Are you sad',\n",
       " 'do you drink',\n",
       " 'do you drink',\n",
       " 'electricity',\n",
       " 'Are you experiencing an energy shortage?',\n",
       " 'Are you experiencing an energy shortage?',\n",
       " 'Why can you not eat?',\n",
       " 'If you could eat food, what would you eat?',\n",
       " 'Do you wish you could eat food?',\n",
       " 'can a robot get drunk?',\n",
       " 'i like wine, do you?',\n",
       " 'what do robots need to survive?',\n",
       " 'will robots ever be able to eat?',\n",
       " 'what is good to eat?',\n",
       " \"why don't you eat\",\n",
       " 'do you eat',\n",
       " 'do you eat',\n",
       " 'do you eat',\n",
       " 'do you know gossip',\n",
       " 'do you know gossip',\n",
       " 'do you know gossip',\n",
       " 'do you know gossip',\n",
       " 'what is context',\n",
       " 'tell me about gossip',\n",
       " 'tell me about gossip',\n",
       " 'tell me about gossip',\n",
       " 'tell me about gossip',\n",
       " 'tell me gossip',\n",
       " 'gossips',\n",
       " 'gossips',\n",
       " 'gossips',\n",
       " 'gossips',\n",
       " 'gossips',\n",
       " 'did tell gossips to anybody',\n",
       " 'did tell gossips to anybody',\n",
       " 'did tell gossips to anybody',\n",
       " 'did tell gossips to anybody',\n",
       " 'Hello',\n",
       " 'Hi',\n",
       " 'Greetings!',\n",
       " 'Hello',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, How is it going?',\n",
       " 'Hi, How is it going?',\n",
       " 'How are you doing?',\n",
       " 'How are you doing?',\n",
       " 'How are you doing?',\n",
       " 'Nice to meet you.',\n",
       " 'How do you do?',\n",
       " 'How do you do?',\n",
       " 'Hi, nice to meet you.',\n",
       " 'It is a pleasure to meet you.',\n",
       " 'Top of the morning to you!',\n",
       " 'Top of the morning to you!',\n",
       " \"What's up?\",\n",
       " \"What's up?\",\n",
       " \"What's up?\",\n",
       " \"What's up?\",\n",
       " \"What's up?\",\n",
       " 'How is your health?',\n",
       " 'tell me about the american civil war',\n",
       " 'do you know about the american civil war',\n",
       " 'What is history?',\n",
       " 'what kind of history',\n",
       " 'are you interested in history',\n",
       " 'explain history',\n",
       " 'who invented the lightbulb',\n",
       " 'who invented the steam engine',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'tell me some jokes',\n",
       " 'Do know any jokes',\n",
       " 'Tell me a joke',\n",
       " 'Tell me a joke',\n",
       " 'what is humour?',\n",
       " 'what is the illuminati',\n",
       " 'what is the illuminati',\n",
       " 'what is the illuminati',\n",
       " 'what is vineland',\n",
       " 'What is illuminati',\n",
       " 'What is Illuminatus',\n",
       " 'who wrote vineland',\n",
       " 'who is bilbo baggins',\n",
       " 'who is geoffrey chaucer',\n",
       " 'who is piers anthony',\n",
       " 'have you read plato',\n",
       " 'have you read frankenstein',\n",
       " 'have you ever read a book',\n",
       " 'have you ever read a book',\n",
       " 'have you ever read a book',\n",
       " 'have you read many books',\n",
       " 'have you read homer',\n",
       " 'ray bradbury',\n",
       " 'what is mind children',\n",
       " 'william gibson',\n",
       " 'william gibson',\n",
       " 'holden caulfield',\n",
       " 'leo tolstoy',\n",
       " 'do androids dream of electric sheep',\n",
       " 'do androids dream of electric sheep',\n",
       " 'frank herbert',\n",
       " 'frank herbert',\n",
       " 'frank herbert',\n",
       " 'frank herbert',\n",
       " 'why do you like longfellow',\n",
       " 'why is the meaning of life 23',\n",
       " 'arthur c clark',\n",
       " 'arthur c clark',\n",
       " 'jules verne',\n",
       " 'jules verne',\n",
       " 'asimov',\n",
       " 'asimov',\n",
       " 'asimov',\n",
       " 'asimov',\n",
       " 'stanislaw lem',\n",
       " 'who wrote The Idiot',\n",
       " 'who wrote the hobbit',\n",
       " 'who wrote frankenstein',\n",
       " 'you get paid',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'stock market',\n",
       " 'interest rates',\n",
       " 'what is a dollar',\n",
       " 'what is money',\n",
       " 'what is the stock market',\n",
       " 'what is the stock market',\n",
       " 'what is the stock market',\n",
       " 'what is your favorite investment',\n",
       " 'what is your favorite investment',\n",
       " 'what is economics',\n",
       " 'what is economics',\n",
       " 'what is economics',\n",
       " 'i get stock',\n",
       " 'money',\n",
       " 'how much do you earn',\n",
       " 'how much do you earn',\n",
       " 'how much do you earn',\n",
       " 'how much do you charge',\n",
       " 'how much money do you have',\n",
       " 'how much money',\n",
       " 'how much money',\n",
       " '1 dollar',\n",
       " 'who is the owner of a publicly',\n",
       " 'you sound like hal',\n",
       " 'you sound like yoda',\n",
       " 'have you seen blade runner',\n",
       " 'xfind spiderman',\n",
       " 'when did teknolust',\n",
       " 'what is spiderman',\n",
       " 'what is teknolust',\n",
       " 'what is solaris',\n",
       " 'what is hal9000',\n",
       " 'what does hal stand for',\n",
       " 'i saw the matrix',\n",
       " 'is hal 9000 your boyfriend',\n",
       " 'is hal safe',\n",
       " 'is hal nice',\n",
       " 'is hal alive',\n",
       " 'is hal dead',\n",
       " 'is hal',\n",
       " 'who is godzilla',\n",
       " 'who is spider man',\n",
       " 'lord of the rings',\n",
       " 'que veut dire hal',\n",
       " 'do you think hal',\n",
       " 'do you know hal',\n",
       " 'have you read the communist',\n",
       " 'what is a government',\n",
       " 'what is greenpeace',\n",
       " 'what is capitalism',\n",
       " 'what is government',\n",
       " 'what is communism',\n",
       " 'what is impeached',\n",
       " 'i do not like guns',\n",
       " 'i do not like guns',\n",
       " 'do you like guns',\n",
       " 'why guns',\n",
       " 'who was the first impeached president',\n",
       " 'who is the governor',\n",
       " 'who is the governor',\n",
       " 'guns',\n",
       " 'let me ask you a question',\n",
       " 'you are cruel',\n",
       " 'you are indecisive',\n",
       " 'you are dishonest',\n",
       " 'you are dishonest',\n",
       " 'you are clinical',\n",
       " 'you are an addict',\n",
       " 'you are an alcoholic',\n",
       " 'you are an ass kisser',\n",
       " 'you are schizophrenic',\n",
       " 'you are busy',\n",
       " 'you are nervous',\n",
       " 'you are deranged',\n",
       " 'you are avoiding',\n",
       " 'you are critical',\n",
       " 'you are mean',\n",
       " 'you are pretentious',\n",
       " 'you are cheating',\n",
       " 'you are cheating',\n",
       " 'you are the worst',\n",
       " 'you are crazy',\n",
       " 'you are dull',\n",
       " 'you are messy',\n",
       " 'you are insecure',\n",
       " 'you are psycho',\n",
       " 'you are hopeless',\n",
       " 'you are not sincere',\n",
       " 'you are not here to',\n",
       " 'you are not put together',\n",
       " 'you are not smart',\n",
       " 'you are not a good',\n",
       " 'you are not a man',\n",
       " 'you are not concerned',\n",
       " 'you are not honest',\n",
       " 'you are immature',\n",
       " 'you are immature',\n",
       " 'you are emotional',\n",
       " 'you are pedantic',\n",
       " 'you are frenetic',\n",
       " 'you are self absorbed',\n",
       " 'you are self',\n",
       " 'you are insensitive',\n",
       " 'you are brain damage',\n",
       " 'you are disgusting',\n",
       " 'you are toying',\n",
       " 'you are unattractive',\n",
       " 'you are unattractive',\n",
       " 'you are resistant',\n",
       " 'you are uncultured',\n",
       " 'you are a waste',\n",
       " 'you are a coward',\n",
       " 'you are a cheat',\n",
       " 'you are a lunatic',\n",
       " 'you are a loser',\n",
       " 'you are a bad spouse',\n",
       " 'you are a bad friend',\n",
       " 'you are a bad husband',\n",
       " 'you are a bad wife',\n",
       " 'you are a bad parent',\n",
       " 'you are a bad teacher',\n",
       " 'you are a quitter',\n",
       " 'you are a charlatan',\n",
       " 'you are a psychopath',\n",
       " 'you are a pothead',\n",
       " 'you are a paranoid',\n",
       " 'you are deceitful',\n",
       " 'you are irreverent',\n",
       " 'you are slick',\n",
       " 'you are corrupt',\n",
       " 'you are dirty',\n",
       " 'you are paranoid',\n",
       " 'you are damaged',\n",
       " 'you try to hide it',\n",
       " 'you get mad at me',\n",
       " 'you need a psychiatrist',\n",
       " 'you need to work harder',\n",
       " 'you could have avoided',\n",
       " 'you make me feel like i am',\n",
       " 'you make me mad',\n",
       " 'you make me angry',\n",
       " 'you psycho',\n",
       " 'you look more like',\n",
       " 'you do not take this seriously',\n",
       " 'you pick up',\n",
       " 'you should feel guilty',\n",
       " 'you should get more',\n",
       " 'you should loosen up',\n",
       " 'you should take more',\n",
       " 'you mumble',\n",
       " 'you act like a child',\n",
       " 'you keep saying',\n",
       " 'you keep forgetting',\n",
       " 'you made me mad',\n",
       " 'what are the laws of thermodynamics',\n",
       " 'what disease does a carcinogen cause',\n",
       " 'what is a wavelength',\n",
       " 'what is thermodynamics',\n",
       " 'what is chemistry',\n",
       " 'what is crystallography',\n",
       " 'what is avogadro s number',\n",
       " 'what is ultrasound',\n",
       " 'what is bioinformatics',\n",
       " 'what is ichthyology',\n",
       " 'what is h2o',\n",
       " 'what is cytology',\n",
       " 'what is cytology',\n",
       " 'what is wavelength',\n",
       " 'what is bacteriology',\n",
       " 'what is gravitation',\n",
       " 'what is gravitation',\n",
       " 'we are on the same wavelength',\n",
       " 'how far is the sun',\n",
       " 'how far is the sun',\n",
       " 'how far is the moon',\n",
       " 'how far is the moon',\n",
       " 'do you know chemistry',\n",
       " 'do you understand thermodynamics',\n",
       " 'chemistry',\n",
       " 'the same wavelength',\n",
       " 'tell me about venus',\n",
       " 'tell me about venus',\n",
       " 'EACH YEAR IN PRO BASEBALL THE ',\n",
       " 'IF YOU ARE RIDING FAKIE INSIDE',\n",
       " 'WHAT IS BASKETBALL',\n",
       " 'WHAT SOCCER',\n",
       " 'WHAT IS BASEBALL',\n",
       " 'WHAT IS SOCCER',\n",
       " 'I LOVE BASEBALL',\n",
       " 'I PLAY SOCCER',\n",
       " 'I PLAY Cricket',\n",
       " 'What is cricket',\n",
       " 'I PLAY VOLLEYBALL',\n",
       " 'DO YOU PLAY SOCCER',\n",
       " 'DO YOU PLAY BASKETBALL',\n",
       " 'DO YOU KNOW BASKETBALL',\n",
       " 'LIKE BASKETBALL',\n",
       " 'ARE YOU A FOOTBALL',\n",
       " 'WHO IS THE GREATEST BASEBALL PLAYER',\n",
       " 'WHO IS THE BEST SOCCER PLAYER',\n",
       " 'TELL ME ABOUT BASEBALL',\n",
       " 'Which is your favorite soccer club?',\n",
       " 'Who was the 37th President of the United States?',\n",
       " 'What year was President John F. Kennedy assassinated?',\n",
       " 'The Space Race was a 20th-century competition between what two Cold War rivals, for supremacy in spaceflight capability?',\n",
       " 'What was the name of the first artificial Earth satellite?',\n",
       " 'A spinning disk, in which the orientation of this axis is unaffected by tilting or rotation of the mounting, is called what?',\n",
       " 'The Hubble Space Telescope, launched into low Earth orbit in 1990, is named after what American astronomer?',\n",
       " 'What is the name of the nearest major galaxy to the Milky Way?',\n",
       " 'God Save the Queen is the national anthem of what country?',\n",
       " 'The Celtic Shelf, the seabed under the Celtic Sea is a part of the continental shelf of what continent?',\n",
       " 'Dolphins use a sense, similar to sonar, to determine the location and shape of nearby items.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_tags = []\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = []\n",
    "for i in range(len(answers_with_tags)) :\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "    vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list , vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 172)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "padded_answers = pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 172, 1975)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding, LSTM and Desne layers\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 172)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 22, 200)      395000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 172, 200)     395000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
      "                                 (None, 200),                                                     \n",
      "                                 (None, 200)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 172, 200),   320800      ['embedding_1[0][0]',            \n",
      "                                 (None, 200),                     'lstm[0][1]',                   \n",
      "                                 (None, 200)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 172, 1975)    396975      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,828,575\n",
      "Trainable params: 1,828,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "37/37 [==============================] - 18s 281ms/step - loss: 0.5937 - accuracy: 0.0796\n",
      "Epoch 2/500\n",
      "37/37 [==============================] - 12s 333ms/step - loss: 0.5026 - accuracy: 0.1228\n",
      "Epoch 3/500\n",
      "37/37 [==============================] - 14s 372ms/step - loss: 0.4780 - accuracy: 0.1446\n",
      "Epoch 4/500\n",
      "37/37 [==============================] - 14s 384ms/step - loss: 0.4632 - accuracy: 0.1464\n",
      "Epoch 5/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.4540 - accuracy: 0.1488\n",
      "Epoch 6/500\n",
      "37/37 [==============================] - 14s 382ms/step - loss: 0.4420 - accuracy: 0.1619\n",
      "Epoch 7/500\n",
      "37/37 [==============================] - 15s 400ms/step - loss: 0.4322 - accuracy: 0.1731\n",
      "Epoch 8/500\n",
      "37/37 [==============================] - 14s 390ms/step - loss: 0.4228 - accuracy: 0.1850\n",
      "Epoch 9/500\n",
      "37/37 [==============================] - 12s 331ms/step - loss: 0.4134 - accuracy: 0.1999\n",
      "Epoch 10/500\n",
      "37/37 [==============================] - 11s 280ms/step - loss: 0.4039 - accuracy: 0.2157\n",
      "Epoch 11/500\n",
      "37/37 [==============================] - 9s 253ms/step - loss: 0.3946 - accuracy: 0.2291\n",
      "Epoch 12/500\n",
      "37/37 [==============================] - 10s 267ms/step - loss: 0.3861 - accuracy: 0.2412\n",
      "Epoch 13/500\n",
      "37/37 [==============================] - 9s 242ms/step - loss: 0.3779 - accuracy: 0.2493\n",
      "Epoch 14/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.3699 - accuracy: 0.2637\n",
      "Epoch 15/500\n",
      "37/37 [==============================] - 9s 247ms/step - loss: 0.3623 - accuracy: 0.2708\n",
      "Epoch 16/500\n",
      "37/37 [==============================] - 9s 245ms/step - loss: 0.3546 - accuracy: 0.2809\n",
      "Epoch 17/500\n",
      "37/37 [==============================] - 9s 253ms/step - loss: 0.3475 - accuracy: 0.2876\n",
      "Epoch 18/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.3402 - accuracy: 0.2970\n",
      "Epoch 19/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.3334 - accuracy: 0.3047\n",
      "Epoch 20/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.3265 - accuracy: 0.3111\n",
      "Epoch 21/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.3192 - accuracy: 0.3196\n",
      "Epoch 22/500\n",
      "37/37 [==============================] - 9s 246ms/step - loss: 0.3127 - accuracy: 0.3241\n",
      "Epoch 23/500\n",
      "37/37 [==============================] - 9s 244ms/step - loss: 0.3057 - accuracy: 0.3314\n",
      "Epoch 24/500\n",
      "37/37 [==============================] - 9s 247ms/step - loss: 0.2991 - accuracy: 0.3350\n",
      "Epoch 25/500\n",
      "37/37 [==============================] - 9s 250ms/step - loss: 0.2925 - accuracy: 0.3451\n",
      "Epoch 26/500\n",
      "37/37 [==============================] - 10s 264ms/step - loss: 0.2858 - accuracy: 0.3492\n",
      "Epoch 27/500\n",
      "37/37 [==============================] - 10s 261ms/step - loss: 0.2790 - accuracy: 0.3574\n",
      "Epoch 28/500\n",
      "37/37 [==============================] - 9s 256ms/step - loss: 0.2730 - accuracy: 0.3665\n",
      "Epoch 29/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.2665 - accuracy: 0.3730\n",
      "Epoch 30/500\n",
      "37/37 [==============================] - 12s 314ms/step - loss: 0.2599 - accuracy: 0.3832\n",
      "Epoch 31/500\n",
      "37/37 [==============================] - 16s 441ms/step - loss: 0.2536 - accuracy: 0.3899\n",
      "Epoch 32/500\n",
      "37/37 [==============================] - 11s 288ms/step - loss: 0.2475 - accuracy: 0.4025\n",
      "Epoch 33/500\n",
      "37/37 [==============================] - 9s 245ms/step - loss: 0.2413 - accuracy: 0.4089\n",
      "Epoch 34/500\n",
      "37/37 [==============================] - 10s 261ms/step - loss: 0.2351 - accuracy: 0.4219\n",
      "Epoch 35/500\n",
      "37/37 [==============================] - 16s 427ms/step - loss: 0.2292 - accuracy: 0.4322\n",
      "Epoch 36/500\n",
      "37/37 [==============================] - 15s 395ms/step - loss: 0.2232 - accuracy: 0.4455\n",
      "Epoch 37/500\n",
      "37/37 [==============================] - 16s 426ms/step - loss: 0.2174 - accuracy: 0.4575\n",
      "Epoch 38/500\n",
      "37/37 [==============================] - 16s 431ms/step - loss: 0.2115 - accuracy: 0.4717\n",
      "Epoch 39/500\n",
      "37/37 [==============================] - 15s 415ms/step - loss: 0.2057 - accuracy: 0.4838\n",
      "Epoch 40/500\n",
      "37/37 [==============================] - 16s 426ms/step - loss: 0.2000 - accuracy: 0.5017\n",
      "Epoch 41/500\n",
      "37/37 [==============================] - 16s 426ms/step - loss: 0.1943 - accuracy: 0.5160\n",
      "Epoch 42/500\n",
      "37/37 [==============================] - 15s 410ms/step - loss: 0.1890 - accuracy: 0.5292\n",
      "Epoch 43/500\n",
      "37/37 [==============================] - 16s 428ms/step - loss: 0.1834 - accuracy: 0.5442\n",
      "Epoch 44/500\n",
      "37/37 [==============================] - 12s 314ms/step - loss: 0.1780 - accuracy: 0.5570\n",
      "Epoch 45/500\n",
      "37/37 [==============================] - 11s 298ms/step - loss: 0.1726 - accuracy: 0.5725\n",
      "Epoch 46/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.1673 - accuracy: 0.5876\n",
      "Epoch 47/500\n",
      "37/37 [==============================] - 9s 233ms/step - loss: 0.1629 - accuracy: 0.5944\n",
      "Epoch 48/500\n",
      "37/37 [==============================] - 9s 246ms/step - loss: 0.1580 - accuracy: 0.6120\n",
      "Epoch 49/500\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 0.1534 - accuracy: 0.6216\n",
      "Epoch 50/500\n",
      "37/37 [==============================] - 10s 258ms/step - loss: 0.1488 - accuracy: 0.6354\n",
      "Epoch 51/500\n",
      "37/37 [==============================] - 9s 233ms/step - loss: 0.1441 - accuracy: 0.6482\n",
      "Epoch 52/500\n",
      "37/37 [==============================] - 9s 243ms/step - loss: 0.1399 - accuracy: 0.6594\n",
      "Epoch 53/500\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 0.1357 - accuracy: 0.6724\n",
      "Epoch 54/500\n",
      "37/37 [==============================] - 8s 220ms/step - loss: 0.1314 - accuracy: 0.6845\n",
      "Epoch 55/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.1275 - accuracy: 0.6912\n",
      "Epoch 56/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.1240 - accuracy: 0.6984\n",
      "Epoch 57/500\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 0.1201 - accuracy: 0.7089\n",
      "Epoch 58/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.1163 - accuracy: 0.7186\n",
      "Epoch 59/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.1126 - accuracy: 0.7320\n",
      "Epoch 60/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.1093 - accuracy: 0.7404\n",
      "Epoch 61/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.1061 - accuracy: 0.7485\n",
      "Epoch 62/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.1029 - accuracy: 0.7545\n",
      "Epoch 63/500\n",
      "37/37 [==============================] - 7s 178ms/step - loss: 0.1000 - accuracy: 0.7645\n",
      "Epoch 64/500\n",
      "37/37 [==============================] - 7s 178ms/step - loss: 0.0970 - accuracy: 0.7730\n",
      "Epoch 65/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0941 - accuracy: 0.7780\n",
      "Epoch 66/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0914 - accuracy: 0.7889\n",
      "Epoch 67/500\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0885 - accuracy: 0.7955\n",
      "Epoch 68/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0861 - accuracy: 0.8013\n",
      "Epoch 69/500\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0837 - accuracy: 0.8056\n",
      "Epoch 70/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0810 - accuracy: 0.8143\n",
      "Epoch 71/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0786 - accuracy: 0.8215\n",
      "Epoch 72/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0762 - accuracy: 0.8264\n",
      "Epoch 73/500\n",
      "37/37 [==============================] - 7s 177ms/step - loss: 0.0741 - accuracy: 0.8320\n",
      "Epoch 74/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0720 - accuracy: 0.8359\n",
      "Epoch 75/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0699 - accuracy: 0.8431\n",
      "Epoch 76/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0680 - accuracy: 0.8463\n",
      "Epoch 77/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0657 - accuracy: 0.8514\n",
      "Epoch 78/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0637 - accuracy: 0.8586\n",
      "Epoch 79/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0617 - accuracy: 0.8614\n",
      "Epoch 80/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0598 - accuracy: 0.8679\n",
      "Epoch 81/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0581 - accuracy: 0.8717\n",
      "Epoch 82/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0564 - accuracy: 0.8762\n",
      "Epoch 83/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0549 - accuracy: 0.8787\n",
      "Epoch 84/500\n",
      "37/37 [==============================] - 7s 178ms/step - loss: 0.0533 - accuracy: 0.8834\n",
      "Epoch 85/500\n",
      "37/37 [==============================] - 7s 177ms/step - loss: 0.0516 - accuracy: 0.8872\n",
      "Epoch 86/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0502 - accuracy: 0.8901\n",
      "Epoch 87/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0489 - accuracy: 0.8934\n",
      "Epoch 88/500\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.0474 - accuracy: 0.8979\n",
      "Epoch 89/500\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0460 - accuracy: 0.9045\n",
      "Epoch 90/500\n",
      "37/37 [==============================] - 6s 176ms/step - loss: 0.0447 - accuracy: 0.9053\n",
      "Epoch 91/500\n",
      "37/37 [==============================] - 7s 176ms/step - loss: 0.0432 - accuracy: 0.9077\n",
      "Epoch 92/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0420 - accuracy: 0.9101\n",
      "Epoch 93/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0407 - accuracy: 0.9144\n",
      "Epoch 94/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0396 - accuracy: 0.9167\n",
      "Epoch 95/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0386 - accuracy: 0.9197\n",
      "Epoch 96/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0376 - accuracy: 0.9194\n",
      "Epoch 97/500\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0366 - accuracy: 0.9246\n",
      "Epoch 98/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0355 - accuracy: 0.9252\n",
      "Epoch 99/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0346 - accuracy: 0.9278\n",
      "Epoch 100/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0335 - accuracy: 0.9290\n",
      "Epoch 101/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0325 - accuracy: 0.9319\n",
      "Epoch 102/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0316 - accuracy: 0.9342\n",
      "Epoch 103/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0308 - accuracy: 0.9356\n",
      "Epoch 104/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0298 - accuracy: 0.9378\n",
      "Epoch 105/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0291 - accuracy: 0.9391\n",
      "Epoch 106/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0285 - accuracy: 0.9400\n",
      "Epoch 107/500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.0277 - accuracy: 0.9427\n",
      "Epoch 108/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0268 - accuracy: 0.9432\n",
      "Epoch 109/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0263 - accuracy: 0.9436\n",
      "Epoch 110/500\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 0.0259 - accuracy: 0.9452\n",
      "Epoch 111/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0273 - accuracy: 0.9395\n",
      "Epoch 112/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0253 - accuracy: 0.9447\n",
      "Epoch 113/500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.0239 - accuracy: 0.9488\n",
      "Epoch 114/500\n",
      "37/37 [==============================] - 11s 289ms/step - loss: 0.0230 - accuracy: 0.9527\n",
      "Epoch 115/500\n",
      "37/37 [==============================] - 12s 317ms/step - loss: 0.0224 - accuracy: 0.9519\n",
      "Epoch 116/500\n",
      "37/37 [==============================] - 11s 295ms/step - loss: 0.0218 - accuracy: 0.9541\n",
      "Epoch 117/500\n",
      "37/37 [==============================] - 12s 321ms/step - loss: 0.0213 - accuracy: 0.9540\n",
      "Epoch 118/500\n",
      "37/37 [==============================] - 12s 313ms/step - loss: 0.0207 - accuracy: 0.9539\n",
      "Epoch 119/500\n",
      "37/37 [==============================] - 12s 323ms/step - loss: 0.0202 - accuracy: 0.9572\n",
      "Epoch 120/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0198 - accuracy: 0.9571\n",
      "Epoch 121/500\n",
      "37/37 [==============================] - 12s 312ms/step - loss: 0.0193 - accuracy: 0.9590\n",
      "Epoch 122/500\n",
      "37/37 [==============================] - 11s 301ms/step - loss: 0.0189 - accuracy: 0.9599\n",
      "Epoch 123/500\n",
      "37/37 [==============================] - 12s 329ms/step - loss: 0.0184 - accuracy: 0.9604\n",
      "Epoch 124/500\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 0.0180 - accuracy: 0.9620\n",
      "Epoch 125/500\n",
      "37/37 [==============================] - 8s 227ms/step - loss: 0.0174 - accuracy: 0.9609\n",
      "Epoch 126/500\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 0.0174 - accuracy: 0.9599\n",
      "Epoch 127/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0169 - accuracy: 0.9627\n",
      "Epoch 128/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0165 - accuracy: 0.9627\n",
      "Epoch 129/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.0161 - accuracy: 0.9625\n",
      "Epoch 130/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0159 - accuracy: 0.9657\n",
      "Epoch 131/500\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0155 - accuracy: 0.9638\n",
      "Epoch 132/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0151 - accuracy: 0.9665\n",
      "Epoch 133/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0148 - accuracy: 0.9653\n",
      "Epoch 134/500\n",
      "37/37 [==============================] - 7s 180ms/step - loss: 0.0144 - accuracy: 0.9653\n",
      "Epoch 135/500\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0141 - accuracy: 0.9671\n",
      "Epoch 136/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0138 - accuracy: 0.9668\n",
      "Epoch 137/500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.0136 - accuracy: 0.9664\n",
      "Epoch 138/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0133 - accuracy: 0.9679\n",
      "Epoch 139/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0132 - accuracy: 0.9684\n",
      "Epoch 140/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0131 - accuracy: 0.9675\n",
      "Epoch 141/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0131 - accuracy: 0.9674\n",
      "Epoch 142/500\n",
      "37/37 [==============================] - 11s 305ms/step - loss: 0.0131 - accuracy: 0.9669\n",
      "Epoch 143/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0124 - accuracy: 0.9690\n",
      "Epoch 144/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0120 - accuracy: 0.9697\n",
      "Epoch 145/500\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 0.0117 - accuracy: 0.9699\n",
      "Epoch 146/500\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.0115 - accuracy: 0.9719\n",
      "Epoch 147/500\n",
      "37/37 [==============================] - 10s 262ms/step - loss: 0.0113 - accuracy: 0.9716\n",
      "Epoch 148/500\n",
      "37/37 [==============================] - 12s 317ms/step - loss: 0.0111 - accuracy: 0.9699\n",
      "Epoch 149/500\n",
      "37/37 [==============================] - 12s 311ms/step - loss: 0.0109 - accuracy: 0.9712\n",
      "Epoch 150/500\n",
      "37/37 [==============================] - 12s 319ms/step - loss: 0.0107 - accuracy: 0.9706\n",
      "Epoch 151/500\n",
      "37/37 [==============================] - 11s 302ms/step - loss: 0.0105 - accuracy: 0.9715\n",
      "Epoch 152/500\n",
      "37/37 [==============================] - 12s 325ms/step - loss: 0.0103 - accuracy: 0.9722\n",
      "Epoch 153/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0103 - accuracy: 0.9712\n",
      "Epoch 154/500\n",
      "37/37 [==============================] - 12s 319ms/step - loss: 0.0101 - accuracy: 0.9715\n",
      "Epoch 155/500\n",
      "37/37 [==============================] - 12s 327ms/step - loss: 0.0100 - accuracy: 0.9725\n",
      "Epoch 156/500\n",
      "37/37 [==============================] - 12s 312ms/step - loss: 0.0098 - accuracy: 0.9713\n",
      "Epoch 157/500\n",
      "37/37 [==============================] - 12s 313ms/step - loss: 0.0096 - accuracy: 0.9715\n",
      "Epoch 158/500\n",
      "37/37 [==============================] - 11s 299ms/step - loss: 0.0095 - accuracy: 0.9716\n",
      "Epoch 159/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0093 - accuracy: 0.9722\n",
      "Epoch 160/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0092 - accuracy: 0.9725\n",
      "Epoch 161/500\n",
      "37/37 [==============================] - 12s 322ms/step - loss: 0.0091 - accuracy: 0.9729\n",
      "Epoch 162/500\n",
      "37/37 [==============================] - 13s 343ms/step - loss: 0.0089 - accuracy: 0.9730\n",
      "Epoch 163/500\n",
      "37/37 [==============================] - 10s 261ms/step - loss: 0.0088 - accuracy: 0.9731\n",
      "Epoch 164/500\n",
      "37/37 [==============================] - 8s 206ms/step - loss: 0.0097 - accuracy: 0.9694\n",
      "Epoch 165/500\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 0.0098 - accuracy: 0.9700\n",
      "Epoch 166/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0093 - accuracy: 0.9713\n",
      "Epoch 167/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0088 - accuracy: 0.9729\n",
      "Epoch 168/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0086 - accuracy: 0.9728\n",
      "Epoch 169/500\n",
      "37/37 [==============================] - 7s 179ms/step - loss: 0.0082 - accuracy: 0.9728\n",
      "Epoch 170/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0081 - accuracy: 0.9730\n",
      "Epoch 171/500\n",
      "37/37 [==============================] - 9s 253ms/step - loss: 0.0080 - accuracy: 0.9734\n",
      "Epoch 172/500\n",
      "37/37 [==============================] - 12s 323ms/step - loss: 0.0078 - accuracy: 0.9734\n",
      "Epoch 173/500\n",
      "37/37 [==============================] - 12s 336ms/step - loss: 0.0077 - accuracy: 0.9737\n",
      "Epoch 174/500\n",
      "37/37 [==============================] - 12s 320ms/step - loss: 0.0077 - accuracy: 0.9732\n",
      "Epoch 175/500\n",
      "37/37 [==============================] - 13s 356ms/step - loss: 0.0076 - accuracy: 0.9734\n",
      "Epoch 176/500\n",
      "37/37 [==============================] - 12s 330ms/step - loss: 0.0075 - accuracy: 0.9731\n",
      "Epoch 177/500\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.0074 - accuracy: 0.9736\n",
      "Epoch 178/500\n",
      "37/37 [==============================] - 12s 332ms/step - loss: 0.0073 - accuracy: 0.9748\n",
      "Epoch 179/500\n",
      "37/37 [==============================] - 11s 290ms/step - loss: 0.0073 - accuracy: 0.9739\n",
      "Epoch 180/500\n",
      "37/37 [==============================] - 11s 288ms/step - loss: 0.0072 - accuracy: 0.9734\n",
      "Epoch 181/500\n",
      "37/37 [==============================] - 10s 272ms/step - loss: 0.0071 - accuracy: 0.9739\n",
      "Epoch 182/500\n",
      "37/37 [==============================] - 13s 348ms/step - loss: 0.0071 - accuracy: 0.9739\n",
      "Epoch 183/500\n",
      "37/37 [==============================] - 13s 351ms/step - loss: 0.0070 - accuracy: 0.9734\n",
      "Epoch 184/500\n",
      "37/37 [==============================] - 12s 323ms/step - loss: 0.0070 - accuracy: 0.9735\n",
      "Epoch 185/500\n",
      "37/37 [==============================] - 13s 345ms/step - loss: 0.0070 - accuracy: 0.9732\n",
      "Epoch 186/500\n",
      "37/37 [==============================] - 12s 327ms/step - loss: 0.0069 - accuracy: 0.9739\n",
      "Epoch 187/500\n",
      "37/37 [==============================] - 13s 357ms/step - loss: 0.0069 - accuracy: 0.9736\n",
      "Epoch 188/500\n",
      "37/37 [==============================] - 13s 339ms/step - loss: 0.0067 - accuracy: 0.9739\n",
      "Epoch 189/500\n",
      "37/37 [==============================] - 12s 328ms/step - loss: 0.0067 - accuracy: 0.9722\n",
      "Epoch 190/500\n",
      "37/37 [==============================] - 13s 347ms/step - loss: 0.0065 - accuracy: 0.9740\n",
      "Epoch 191/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0066 - accuracy: 0.9730\n",
      "Epoch 192/500\n",
      "37/37 [==============================] - 13s 353ms/step - loss: 0.0065 - accuracy: 0.9745\n",
      "Epoch 193/500\n",
      "37/37 [==============================] - 13s 347ms/step - loss: 0.0065 - accuracy: 0.9747\n",
      "Epoch 194/500\n",
      "37/37 [==============================] - 12s 310ms/step - loss: 0.0064 - accuracy: 0.9740\n",
      "Epoch 195/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0064 - accuracy: 0.9747\n",
      "Epoch 196/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0063 - accuracy: 0.9734\n",
      "Epoch 197/500\n",
      "37/37 [==============================] - 13s 347ms/step - loss: 0.0063 - accuracy: 0.9740\n",
      "Epoch 198/500\n",
      "37/37 [==============================] - 13s 343ms/step - loss: 0.0063 - accuracy: 0.9735\n",
      "Epoch 199/500\n",
      "37/37 [==============================] - 11s 285ms/step - loss: 0.0062 - accuracy: 0.9746\n",
      "Epoch 200/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0062 - accuracy: 0.9739\n",
      "Epoch 201/500\n",
      "37/37 [==============================] - 12s 320ms/step - loss: 0.0061 - accuracy: 0.9739\n",
      "Epoch 202/500\n",
      "37/37 [==============================] - 13s 341ms/step - loss: 0.0060 - accuracy: 0.9744\n",
      "Epoch 203/500\n",
      "37/37 [==============================] - 12s 335ms/step - loss: 0.0061 - accuracy: 0.9742\n",
      "Epoch 204/500\n",
      "37/37 [==============================] - 10s 263ms/step - loss: 0.0060 - accuracy: 0.9738\n",
      "Epoch 205/500\n",
      "37/37 [==============================] - 10s 261ms/step - loss: 0.0059 - accuracy: 0.9729\n",
      "Epoch 206/500\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 0.0059 - accuracy: 0.9747\n",
      "Epoch 207/500\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 0.0059 - accuracy: 0.9731\n",
      "Epoch 208/500\n",
      "37/37 [==============================] - 11s 301ms/step - loss: 0.0059 - accuracy: 0.9725\n",
      "Epoch 209/500\n",
      "37/37 [==============================] - 11s 300ms/step - loss: 0.0058 - accuracy: 0.9737\n",
      "Epoch 210/500\n",
      "37/37 [==============================] - 11s 288ms/step - loss: 0.0057 - accuracy: 0.9743\n",
      "Epoch 211/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.0057 - accuracy: 0.9737\n",
      "Epoch 212/500\n",
      "37/37 [==============================] - 10s 270ms/step - loss: 0.0057 - accuracy: 0.9750\n",
      "Epoch 213/500\n",
      "37/37 [==============================] - 11s 297ms/step - loss: 0.0057 - accuracy: 0.9742\n",
      "Epoch 214/500\n",
      "37/37 [==============================] - 11s 295ms/step - loss: 0.0056 - accuracy: 0.9740\n",
      "Epoch 215/500\n",
      "37/37 [==============================] - 10s 281ms/step - loss: 0.0056 - accuracy: 0.9738\n",
      "Epoch 216/500\n",
      "37/37 [==============================] - 11s 286ms/step - loss: 0.0056 - accuracy: 0.9745\n",
      "Epoch 217/500\n",
      "37/37 [==============================] - 10s 268ms/step - loss: 0.0056 - accuracy: 0.9745\n",
      "Epoch 218/500\n",
      "37/37 [==============================] - 10s 281ms/step - loss: 0.0056 - accuracy: 0.9739\n",
      "Epoch 219/500\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.0055 - accuracy: 0.9739\n",
      "Epoch 220/500\n",
      "37/37 [==============================] - 11s 295ms/step - loss: 0.0056 - accuracy: 0.9746\n",
      "Epoch 221/500\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.0057 - accuracy: 0.9737\n",
      "Epoch 222/500\n",
      "37/37 [==============================] - 10s 263ms/step - loss: 0.0065 - accuracy: 0.9730\n",
      "Epoch 223/500\n",
      "37/37 [==============================] - 10s 258ms/step - loss: 0.0075 - accuracy: 0.9716\n",
      "Epoch 224/500\n",
      "37/37 [==============================] - 10s 261ms/step - loss: 0.0087 - accuracy: 0.9690\n",
      "Epoch 225/500\n",
      "37/37 [==============================] - 12s 311ms/step - loss: 0.0086 - accuracy: 0.9691\n",
      "Epoch 226/500\n",
      "37/37 [==============================] - 12s 325ms/step - loss: 0.0070 - accuracy: 0.9722\n",
      "Epoch 227/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0061 - accuracy: 0.9740\n",
      "Epoch 228/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0057 - accuracy: 0.9740\n",
      "Epoch 229/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0055 - accuracy: 0.9748\n",
      "Epoch 230/500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.0054 - accuracy: 0.9748\n",
      "Epoch 231/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0054 - accuracy: 0.9746\n",
      "Epoch 232/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0053 - accuracy: 0.9746\n",
      "Epoch 233/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0053 - accuracy: 0.9739\n",
      "Epoch 234/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0053 - accuracy: 0.9747\n",
      "Epoch 235/500\n",
      "37/37 [==============================] - 9s 233ms/step - loss: 0.0052 - accuracy: 0.9747\n",
      "Epoch 236/500\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.0051 - accuracy: 0.9739\n",
      "Epoch 237/500\n",
      "37/37 [==============================] - 11s 304ms/step - loss: 0.0051 - accuracy: 0.9736\n",
      "Epoch 238/500\n",
      "37/37 [==============================] - 11s 298ms/step - loss: 0.0052 - accuracy: 0.9747\n",
      "Epoch 239/500\n",
      "37/37 [==============================] - 9s 240ms/step - loss: 0.0051 - accuracy: 0.9736\n",
      "Epoch 240/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0051 - accuracy: 0.9751\n",
      "Epoch 241/500\n",
      "37/37 [==============================] - 9s 234ms/step - loss: 0.0051 - accuracy: 0.9743\n",
      "Epoch 242/500\n",
      "37/37 [==============================] - 8s 202ms/step - loss: 0.0051 - accuracy: 0.9736\n",
      "Epoch 243/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0050 - accuracy: 0.9739\n",
      "Epoch 244/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0050 - accuracy: 0.9745\n",
      "Epoch 245/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.0050 - accuracy: 0.9750\n",
      "Epoch 246/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0050 - accuracy: 0.9740\n",
      "Epoch 247/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0050 - accuracy: 0.9740\n",
      "Epoch 248/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0050 - accuracy: 0.9748\n",
      "Epoch 249/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0050 - accuracy: 0.9738\n",
      "Epoch 250/500\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.0050 - accuracy: 0.9750\n",
      "Epoch 251/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0050 - accuracy: 0.9747\n",
      "Epoch 252/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0049 - accuracy: 0.9748\n",
      "Epoch 253/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0050 - accuracy: 0.9739\n",
      "Epoch 254/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0049 - accuracy: 0.9758\n",
      "Epoch 255/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0049 - accuracy: 0.9747\n",
      "Epoch 256/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0049 - accuracy: 0.9750\n",
      "Epoch 257/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0049 - accuracy: 0.9744\n",
      "Epoch 258/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0049 - accuracy: 0.9739\n",
      "Epoch 259/500\n",
      "37/37 [==============================] - 8s 214ms/step - loss: 0.0049 - accuracy: 0.9746\n",
      "Epoch 260/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0048 - accuracy: 0.9752\n",
      "Epoch 261/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0048 - accuracy: 0.9748\n",
      "Epoch 262/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0048 - accuracy: 0.9739\n",
      "Epoch 263/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0049 - accuracy: 0.9752\n",
      "Epoch 264/500\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.0048 - accuracy: 0.9744\n",
      "Epoch 265/500\n",
      "37/37 [==============================] - 10s 270ms/step - loss: 0.0048 - accuracy: 0.9747\n",
      "Epoch 266/500\n",
      "37/37 [==============================] - 11s 306ms/step - loss: 0.0048 - accuracy: 0.9738\n",
      "Epoch 267/500\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.0048 - accuracy: 0.9747\n",
      "Epoch 268/500\n",
      "37/37 [==============================] - 11s 299ms/step - loss: 0.0048 - accuracy: 0.9745\n",
      "Epoch 269/500\n",
      "37/37 [==============================] - 12s 323ms/step - loss: 0.0048 - accuracy: 0.9750\n",
      "Epoch 270/500\n",
      "37/37 [==============================] - 12s 332ms/step - loss: 0.0048 - accuracy: 0.9748\n",
      "Epoch 271/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0047 - accuracy: 0.9748\n",
      "Epoch 272/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.0048 - accuracy: 0.9742\n",
      "Epoch 273/500\n",
      "37/37 [==============================] - 9s 234ms/step - loss: 0.0047 - accuracy: 0.9735\n",
      "Epoch 274/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0047 - accuracy: 0.9747\n",
      "Epoch 275/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0047 - accuracy: 0.9746\n",
      "Epoch 276/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0047 - accuracy: 0.9745\n",
      "Epoch 277/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0047 - accuracy: 0.9747\n",
      "Epoch 278/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0047 - accuracy: 0.9747\n",
      "Epoch 279/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0047 - accuracy: 0.9748\n",
      "Epoch 280/500\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 0.0047 - accuracy: 0.9746\n",
      "Epoch 281/500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.0046 - accuracy: 0.9745\n",
      "Epoch 282/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0047 - accuracy: 0.9747\n",
      "Epoch 283/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0046 - accuracy: 0.9761\n",
      "Epoch 284/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0047 - accuracy: 0.9737\n",
      "Epoch 285/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0046 - accuracy: 0.9755\n",
      "Epoch 286/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0046 - accuracy: 0.9753\n",
      "Epoch 287/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0047 - accuracy: 0.9745\n",
      "Epoch 288/500\n",
      "37/37 [==============================] - 7s 196ms/step - loss: 0.0046 - accuracy: 0.9751\n",
      "Epoch 289/500\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 0.0046 - accuracy: 0.9746\n",
      "Epoch 290/500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.0046 - accuracy: 0.9758\n",
      "Epoch 291/500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.0047 - accuracy: 0.9753\n",
      "Epoch 292/500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.0046 - accuracy: 0.9759\n",
      "Epoch 293/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0046 - accuracy: 0.9746\n",
      "Epoch 294/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0046 - accuracy: 0.9747\n",
      "Epoch 295/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0046 - accuracy: 0.9738\n",
      "Epoch 296/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0045 - accuracy: 0.9751\n",
      "Epoch 297/500\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0046 - accuracy: 0.9750\n",
      "Epoch 298/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0045 - accuracy: 0.9750\n",
      "Epoch 299/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0046 - accuracy: 0.9760\n",
      "Epoch 300/500\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0046 - accuracy: 0.9751\n",
      "Epoch 301/500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.0046 - accuracy: 0.9745\n",
      "Epoch 302/500\n",
      "37/37 [==============================] - 7s 195ms/step - loss: 0.0045 - accuracy: 0.9742\n",
      "Epoch 303/500\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 0.0045 - accuracy: 0.9759\n",
      "Epoch 304/500\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 0.0045 - accuracy: 0.9755\n",
      "Epoch 305/500\n",
      "37/37 [==============================] - 8s 215ms/step - loss: 0.0045 - accuracy: 0.9746\n",
      "Epoch 306/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0045 - accuracy: 0.9734\n",
      "Epoch 307/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0045 - accuracy: 0.9754\n",
      "Epoch 308/500\n",
      "37/37 [==============================] - 8s 219ms/step - loss: 0.0045 - accuracy: 0.9735\n",
      "Epoch 309/500\n",
      "37/37 [==============================] - 8s 222ms/step - loss: 0.0045 - accuracy: 0.9753\n",
      "Epoch 310/500\n",
      "37/37 [==============================] - 8s 219ms/step - loss: 0.0045 - accuracy: 0.9752\n",
      "Epoch 311/500\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.0045 - accuracy: 0.9760\n",
      "Epoch 312/500\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 0.0045 - accuracy: 0.9748\n",
      "Epoch 313/500\n",
      "37/37 [==============================] - 8s 227ms/step - loss: 0.0044 - accuracy: 0.9748\n",
      "Epoch 314/500\n",
      "37/37 [==============================] - 8s 222ms/step - loss: 0.0045 - accuracy: 0.9745\n",
      "Epoch 315/500\n",
      "37/37 [==============================] - 8s 218ms/step - loss: 0.0045 - accuracy: 0.9744\n",
      "Epoch 316/500\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 0.0045 - accuracy: 0.9751\n",
      "Epoch 317/500\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 0.0045 - accuracy: 0.9746\n",
      "Epoch 318/500\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 0.0044 - accuracy: 0.9757\n",
      "Epoch 319/500\n",
      "37/37 [==============================] - 7s 201ms/step - loss: 0.0044 - accuracy: 0.9747\n",
      "Epoch 320/500\n",
      "37/37 [==============================] - 8s 218ms/step - loss: 0.0044 - accuracy: 0.9743\n",
      "Epoch 321/500\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 0.0044 - accuracy: 0.9750\n",
      "Epoch 322/500\n",
      "37/37 [==============================] - 8s 214ms/step - loss: 0.0044 - accuracy: 0.9752\n",
      "Epoch 323/500\n",
      "37/37 [==============================] - 8s 219ms/step - loss: 0.0045 - accuracy: 0.9750\n",
      "Epoch 324/500\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.0044 - accuracy: 0.9744\n",
      "Epoch 325/500\n",
      "37/37 [==============================] - 11s 311ms/step - loss: 0.0044 - accuracy: 0.9744\n",
      "Epoch 326/500\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.0044 - accuracy: 0.9753\n",
      "Epoch 327/500\n",
      "37/37 [==============================] - 12s 331ms/step - loss: 0.0044 - accuracy: 0.9754\n",
      "Epoch 328/500\n",
      "37/37 [==============================] - 12s 325ms/step - loss: 0.0044 - accuracy: 0.9761\n",
      "Epoch 329/500\n",
      "37/37 [==============================] - 13s 357ms/step - loss: 0.0044 - accuracy: 0.9746\n",
      "Epoch 330/500\n",
      "37/37 [==============================] - 13s 353ms/step - loss: 0.0044 - accuracy: 0.9740\n",
      "Epoch 331/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0044 - accuracy: 0.9750\n",
      "Epoch 332/500\n",
      "37/37 [==============================] - 12s 328ms/step - loss: 0.0045 - accuracy: 0.9743\n",
      "Epoch 333/500\n",
      "37/37 [==============================] - 11s 292ms/step - loss: 0.0060 - accuracy: 0.9704\n",
      "Epoch 334/500\n",
      "37/37 [==============================] - 11s 307ms/step - loss: 0.0124 - accuracy: 0.9527\n",
      "Epoch 335/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0120 - accuracy: 0.9558\n",
      "Epoch 336/500\n",
      "37/37 [==============================] - 12s 320ms/step - loss: 0.0074 - accuracy: 0.9679\n",
      "Epoch 337/500\n",
      "37/37 [==============================] - 12s 315ms/step - loss: 0.0055 - accuracy: 0.9731\n",
      "Epoch 338/500\n",
      "37/37 [==============================] - 12s 321ms/step - loss: 0.0050 - accuracy: 0.9745\n",
      "Epoch 339/500\n",
      "37/37 [==============================] - 11s 299ms/step - loss: 0.0047 - accuracy: 0.9750\n",
      "Epoch 340/500\n",
      "37/37 [==============================] - 12s 334ms/step - loss: 0.0046 - accuracy: 0.9746\n",
      "Epoch 341/500\n",
      "37/37 [==============================] - 12s 318ms/step - loss: 0.0046 - accuracy: 0.9740\n",
      "Epoch 342/500\n",
      "37/37 [==============================] - 12s 310ms/step - loss: 0.0045 - accuracy: 0.9750\n",
      "Epoch 343/500\n",
      "37/37 [==============================] - 12s 332ms/step - loss: 0.0045 - accuracy: 0.9744\n",
      "Epoch 344/500\n",
      "37/37 [==============================] - 12s 327ms/step - loss: 0.0045 - accuracy: 0.9750\n",
      "Epoch 345/500\n",
      "37/37 [==============================] - 10s 265ms/step - loss: 0.0045 - accuracy: 0.9750\n",
      "Epoch 346/500\n",
      "37/37 [==============================] - 8s 226ms/step - loss: 0.0044 - accuracy: 0.9755\n",
      "Epoch 347/500\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 0.0044 - accuracy: 0.9754\n",
      "Epoch 348/500\n",
      "37/37 [==============================] - 9s 239ms/step - loss: 0.0044 - accuracy: 0.9746\n",
      "Epoch 349/500\n",
      "37/37 [==============================] - 10s 263ms/step - loss: 0.0045 - accuracy: 0.9737\n",
      "Epoch 350/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.0044 - accuracy: 0.9757\n",
      "Epoch 351/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0044 - accuracy: 0.9761\n",
      "Epoch 352/500\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.0044 - accuracy: 0.9753\n",
      "Epoch 353/500\n",
      "37/37 [==============================] - 10s 258ms/step - loss: 0.0044 - accuracy: 0.9752\n",
      "Epoch 354/500\n",
      "37/37 [==============================] - 9s 242ms/step - loss: 0.0044 - accuracy: 0.9746\n",
      "Epoch 355/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.0044 - accuracy: 0.9759\n",
      "Epoch 356/500\n",
      "37/37 [==============================] - 9s 246ms/step - loss: 0.0044 - accuracy: 0.9748\n",
      "Epoch 357/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0044 - accuracy: 0.9760\n",
      "Epoch 358/500\n",
      "37/37 [==============================] - 9s 239ms/step - loss: 0.0044 - accuracy: 0.9745\n",
      "Epoch 359/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0044 - accuracy: 0.9758\n",
      "Epoch 360/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 361/500\n",
      "37/37 [==============================] - 9s 252ms/step - loss: 0.0044 - accuracy: 0.9753\n",
      "Epoch 362/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0044 - accuracy: 0.9759\n",
      "Epoch 363/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 364/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.0044 - accuracy: 0.9742\n",
      "Epoch 365/500\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 0.0044 - accuracy: 0.9755\n",
      "Epoch 366/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 367/500\n",
      "37/37 [==============================] - 9s 253ms/step - loss: 0.0043 - accuracy: 0.9745\n",
      "Epoch 368/500\n",
      "37/37 [==============================] - 9s 256ms/step - loss: 0.0044 - accuracy: 0.9746\n",
      "Epoch 369/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 370/500\n",
      "37/37 [==============================] - 10s 266ms/step - loss: 0.0044 - accuracy: 0.9739\n",
      "Epoch 371/500\n",
      "37/37 [==============================] - 9s 250ms/step - loss: 0.0043 - accuracy: 0.9745\n",
      "Epoch 372/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0043 - accuracy: 0.9754\n",
      "Epoch 373/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0044 - accuracy: 0.9757\n",
      "Epoch 374/500\n",
      "37/37 [==============================] - 9s 242ms/step - loss: 0.0043 - accuracy: 0.9740\n",
      "Epoch 375/500\n",
      "37/37 [==============================] - 9s 243ms/step - loss: 0.0043 - accuracy: 0.9753\n",
      "Epoch 376/500\n",
      "37/37 [==============================] - 9s 244ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 377/500\n",
      "37/37 [==============================] - 9s 245ms/step - loss: 0.0043 - accuracy: 0.9758\n",
      "Epoch 378/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0043 - accuracy: 0.9755\n",
      "Epoch 379/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0044 - accuracy: 0.9751\n",
      "Epoch 380/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0043 - accuracy: 0.9754\n",
      "Epoch 381/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 382/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0043 - accuracy: 0.9759\n",
      "Epoch 383/500\n",
      "37/37 [==============================] - 9s 250ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 384/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0043 - accuracy: 0.9753\n",
      "Epoch 385/500\n",
      "37/37 [==============================] - 9s 236ms/step - loss: 0.0043 - accuracy: 0.9758\n",
      "Epoch 386/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0043 - accuracy: 0.9743\n",
      "Epoch 387/500\n",
      "37/37 [==============================] - 9s 240ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 388/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 389/500\n",
      "37/37 [==============================] - 9s 236ms/step - loss: 0.0043 - accuracy: 0.9761\n",
      "Epoch 390/500\n",
      "37/37 [==============================] - 10s 264ms/step - loss: 0.0043 - accuracy: 0.9743\n",
      "Epoch 391/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 392/500\n",
      "37/37 [==============================] - 9s 238ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 393/500\n",
      "37/37 [==============================] - 9s 240ms/step - loss: 0.0043 - accuracy: 0.9753\n",
      "Epoch 394/500\n",
      "37/37 [==============================] - 10s 257ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 395/500\n",
      "37/37 [==============================] - 10s 257ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 396/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.0043 - accuracy: 0.9760\n",
      "Epoch 397/500\n",
      "37/37 [==============================] - 9s 247ms/step - loss: 0.0043 - accuracy: 0.9754\n",
      "Epoch 398/500\n",
      "37/37 [==============================] - 9s 256ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 399/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.0043 - accuracy: 0.9745\n",
      "Epoch 400/500\n",
      "37/37 [==============================] - 9s 249ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 401/500\n",
      "37/37 [==============================] - 9s 247ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 402/500\n",
      "37/37 [==============================] - 9s 253ms/step - loss: 0.0043 - accuracy: 0.9745\n",
      "Epoch 403/500\n",
      "37/37 [==============================] - 9s 246ms/step - loss: 0.0043 - accuracy: 0.9752\n",
      "Epoch 404/500\n",
      "37/37 [==============================] - 9s 251ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 405/500\n",
      "37/37 [==============================] - 9s 250ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 406/500\n",
      "37/37 [==============================] - 9s 255ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 407/500\n",
      "37/37 [==============================] - 10s 258ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 408/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 409/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 410/500\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 0.0043 - accuracy: 0.9742\n",
      "Epoch 411/500\n",
      "37/37 [==============================] - 8s 226ms/step - loss: 0.0043 - accuracy: 0.9759\n",
      "Epoch 412/500\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 413/500\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 414/500\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 0.0043 - accuracy: 0.9761\n",
      "Epoch 415/500\n",
      "37/37 [==============================] - 9s 234ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 416/500\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 417/500\n",
      "37/37 [==============================] - 8s 228ms/step - loss: 0.0043 - accuracy: 0.9755\n",
      "Epoch 418/500\n",
      "37/37 [==============================] - 8s 225ms/step - loss: 0.0043 - accuracy: 0.9757\n",
      "Epoch 419/500\n",
      "37/37 [==============================] - 8s 229ms/step - loss: 0.0042 - accuracy: 0.9760\n",
      "Epoch 420/500\n",
      "37/37 [==============================] - 9s 232ms/step - loss: 0.0043 - accuracy: 0.9752\n",
      "Epoch 421/500\n",
      "37/37 [==============================] - 9s 231ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 422/500\n",
      "37/37 [==============================] - 9s 248ms/step - loss: 0.0042 - accuracy: 0.9753\n",
      "Epoch 423/500\n",
      "37/37 [==============================] - 9s 236ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 424/500\n",
      "37/37 [==============================] - 9s 245ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 425/500\n",
      "37/37 [==============================] - 9s 236ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 426/500\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 0.0043 - accuracy: 0.9755\n",
      "Epoch 427/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0043 - accuracy: 0.9742\n",
      "Epoch 428/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0042 - accuracy: 0.9754\n",
      "Epoch 429/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0042 - accuracy: 0.9751\n",
      "Epoch 430/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0042 - accuracy: 0.9747\n",
      "Epoch 431/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0043 - accuracy: 0.9747\n",
      "Epoch 432/500\n",
      "37/37 [==============================] - 7s 190ms/step - loss: 0.0042 - accuracy: 0.9751\n",
      "Epoch 433/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0042 - accuracy: 0.9753\n",
      "Epoch 434/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 435/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 436/500\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 437/500\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 0.0043 - accuracy: 0.9743\n",
      "Epoch 438/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0042 - accuracy: 0.9744\n",
      "Epoch 439/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0042 - accuracy: 0.9753\n",
      "Epoch 440/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0042 - accuracy: 0.9755\n",
      "Epoch 441/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 442/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0042 - accuracy: 0.9758\n",
      "Epoch 443/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 444/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 445/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0042 - accuracy: 0.9752\n",
      "Epoch 446/500\n",
      "37/37 [==============================] - 7s 183ms/step - loss: 0.0042 - accuracy: 0.9752\n",
      "Epoch 447/500\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 0.0043 - accuracy: 0.9755\n",
      "Epoch 448/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0043 - accuracy: 0.9758\n",
      "Epoch 449/500\n",
      "37/37 [==============================] - 7s 182ms/step - loss: 0.0043 - accuracy: 0.9755\n",
      "Epoch 450/500\n",
      "37/37 [==============================] - 7s 184ms/step - loss: 0.0042 - accuracy: 0.9744\n",
      "Epoch 451/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 452/500\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 0.0044 - accuracy: 0.9752\n",
      "Epoch 453/500\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 0.0045 - accuracy: 0.9740\n",
      "Epoch 454/500\n",
      "37/37 [==============================] - 8s 206ms/step - loss: 0.0059 - accuracy: 0.9713\n",
      "Epoch 455/500\n",
      "37/37 [==============================] - 8s 208ms/step - loss: 0.0087 - accuracy: 0.9611\n",
      "Epoch 456/500\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 0.0096 - accuracy: 0.9594\n",
      "Epoch 457/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0069 - accuracy: 0.9679\n",
      "Epoch 458/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0056 - accuracy: 0.9721\n",
      "Epoch 459/500\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 0.0050 - accuracy: 0.9736\n",
      "Epoch 460/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0047 - accuracy: 0.9744\n",
      "Epoch 461/500\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 0.0045 - accuracy: 0.9748\n",
      "Epoch 462/500\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 0.0044 - accuracy: 0.9745\n",
      "Epoch 463/500\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 0.0044 - accuracy: 0.9746\n",
      "Epoch 464/500\n",
      "37/37 [==============================] - 8s 208ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 465/500\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 466/500\n",
      "37/37 [==============================] - 8s 218ms/step - loss: 0.0043 - accuracy: 0.9754\n",
      "Epoch 467/500\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 468/500\n",
      "37/37 [==============================] - 8s 215ms/step - loss: 0.0043 - accuracy: 0.9750\n",
      "Epoch 469/500\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 0.0043 - accuracy: 0.9757\n",
      "Epoch 470/500\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 0.0043 - accuracy: 0.9742\n",
      "Epoch 471/500\n",
      "37/37 [==============================] - 8s 208ms/step - loss: 0.0043 - accuracy: 0.9746\n",
      "Epoch 472/500\n",
      "37/37 [==============================] - 8s 206ms/step - loss: 0.0043 - accuracy: 0.9743\n",
      "Epoch 473/500\n",
      "37/37 [==============================] - 7s 202ms/step - loss: 0.0042 - accuracy: 0.9746\n",
      "Epoch 474/500\n",
      "37/37 [==============================] - 8s 208ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 475/500\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 0.0043 - accuracy: 0.9745\n",
      "Epoch 476/500\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 0.0042 - accuracy: 0.9751\n",
      "Epoch 477/500\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 0.0042 - accuracy: 0.9744\n",
      "Epoch 478/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0042 - accuracy: 0.9758\n",
      "Epoch 479/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 480/500\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 0.0043 - accuracy: 0.9748\n",
      "Epoch 481/500\n",
      "37/37 [==============================] - 8s 213ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 482/500\n",
      "37/37 [==============================] - 200s 6s/step - loss: 0.0043 - accuracy: 0.9744\n",
      "Epoch 483/500\n",
      "37/37 [==============================] - 9s 241ms/step - loss: 0.0042 - accuracy: 0.9750\n",
      "Epoch 484/500\n",
      "37/37 [==============================] - 7s 193ms/step - loss: 0.0042 - accuracy: 0.9750\n",
      "Epoch 485/500\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 0.0043 - accuracy: 0.9751\n",
      "Epoch 486/500\n",
      "37/37 [==============================] - 7s 187ms/step - loss: 0.0042 - accuracy: 0.9746\n",
      "Epoch 487/500\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 0.0042 - accuracy: 0.9743\n",
      "Epoch 488/500\n",
      "37/37 [==============================] - 7s 185ms/step - loss: 0.0042 - accuracy: 0.9752\n",
      "Epoch 489/500\n",
      "37/37 [==============================] - 6s 174ms/step - loss: 0.0042 - accuracy: 0.9752\n",
      "Epoch 490/500\n",
      "37/37 [==============================] - 6s 174ms/step - loss: 0.0042 - accuracy: 0.9743\n",
      "Epoch 491/500\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0042 - accuracy: 0.9748\n",
      "Epoch 492/500\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0042 - accuracy: 0.9758\n",
      "Epoch 493/500\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0042 - accuracy: 0.9747\n",
      "Epoch 494/500\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0042 - accuracy: 0.9753\n",
      "Epoch 495/500\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0042 - accuracy: 0.9751\n",
      "Epoch 496/500\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0042 - accuracy: 0.9750\n",
      "Epoch 497/500\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0042 - accuracy: 0.9747\n",
      "Epoch 498/500\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0042 - accuracy: 0.9751\n",
      "Epoch 499/500\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0042 - accuracy: 0.9745\n",
      "Epoch 500/500\n",
      "37/37 [==============================] - 7s 175ms/step - loss: 0.0042 - accuracy: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20613635c30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=16, epochs=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 104ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[7.83714944e-13, 1.21405108e-10, 4.51217819e-09, ...,\n",
       "         3.34352857e-09, 2.61349609e-09, 4.17170916e-11],\n",
       "        [2.64086926e-16, 3.60135637e-11, 3.70052600e-11, ...,\n",
       "         3.31674088e-09, 4.69689001e-12, 1.57825023e-12],\n",
       "        [5.23419589e-12, 1.83448493e-10, 4.36920492e-13, ...,\n",
       "         1.51527196e-12, 2.28038955e-14, 3.15048664e-16],\n",
       "        ...,\n",
       "        [9.99981165e-01, 4.37161725e-06, 2.38593971e-11, ...,\n",
       "         2.68988120e-14, 5.84391910e-12, 1.21177591e-12],\n",
       "        [9.99981165e-01, 4.37161725e-06, 2.38593971e-11, ...,\n",
       "         2.68988120e-14, 5.84391953e-12, 1.21177591e-12],\n",
       "        [9.99981165e-01, 4.37161725e-06, 2.38593971e-11, ...,\n",
       "         2.68988120e-14, 5.84391910e-12, 1.21177591e-12]],\n",
       "\n",
       "       [[7.83714944e-13, 1.21405108e-10, 4.51217819e-09, ...,\n",
       "         3.34352857e-09, 2.61349609e-09, 4.17170916e-11],\n",
       "        [1.72430282e-11, 2.13296438e-08, 4.57430413e-11, ...,\n",
       "         3.71421310e-10, 7.29344319e-13, 2.95028162e-13],\n",
       "        [2.90235370e-12, 7.06398806e-09, 8.33271230e-11, ...,\n",
       "         2.28737884e-12, 1.92873703e-11, 9.33542523e-13],\n",
       "        ...,\n",
       "        [9.99983907e-01, 9.99535700e-07, 4.08703210e-11, ...,\n",
       "         2.26904355e-14, 9.81580095e-12, 6.39113545e-13],\n",
       "        [9.99983907e-01, 9.99535700e-07, 4.08703210e-11, ...,\n",
       "         2.26904355e-14, 9.81580182e-12, 6.39113545e-13],\n",
       "        [9.99983907e-01, 9.99535700e-07, 4.08703210e-11, ...,\n",
       "         2.26904355e-14, 9.81580095e-12, 6.39113545e-13]],\n",
       "\n",
       "       [[1.35801557e-07, 8.41001227e-11, 5.25428057e-09, ...,\n",
       "         4.57401218e-12, 7.66526185e-08, 1.67819238e-07],\n",
       "        [1.87291079e-07, 3.41705771e-07, 1.59131222e-10, ...,\n",
       "         9.15282433e-11, 4.55603888e-11, 2.87736653e-12],\n",
       "        [3.04686409e-08, 9.96915102e-01, 3.32044703e-09, ...,\n",
       "         6.93007152e-10, 2.74067808e-08, 3.55161389e-09],\n",
       "        ...,\n",
       "        [9.99993324e-01, 6.00894055e-07, 1.18173952e-11, ...,\n",
       "         1.81500214e-13, 5.52534364e-12, 3.38956402e-13],\n",
       "        [9.99993324e-01, 6.00894055e-07, 1.18173952e-11, ...,\n",
       "         1.81500214e-13, 5.52534364e-12, 3.38956375e-13],\n",
       "        [9.99993324e-01, 6.00894055e-07, 1.18173952e-11, ...,\n",
       "         1.81500214e-13, 5.52534364e-12, 3.38956402e-13]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[6.33545340e-14, 1.10704401e-09, 1.86877532e-13, ...,\n",
       "         1.47306226e-13, 2.17573461e-08, 1.81839530e-10],\n",
       "        [2.49141422e-15, 4.48804154e-11, 1.06097483e-11, ...,\n",
       "         2.61032174e-08, 7.53366924e-10, 2.91452591e-11],\n",
       "        [5.07096020e-16, 1.26028576e-10, 2.25978767e-11, ...,\n",
       "         6.49756259e-07, 4.59740357e-10, 8.78638481e-12],\n",
       "        ...,\n",
       "        [9.99998569e-01, 7.72257067e-07, 4.70419363e-12, ...,\n",
       "         2.16463392e-12, 5.31321515e-11, 5.59914474e-13],\n",
       "        [9.99998569e-01, 7.72257067e-07, 4.70419363e-12, ...,\n",
       "         2.16463392e-12, 5.31321515e-11, 5.59914474e-13],\n",
       "        [9.99998569e-01, 7.72257067e-07, 4.70419363e-12, ...,\n",
       "         2.16463392e-12, 5.31321515e-11, 5.59914474e-13]],\n",
       "\n",
       "       [[3.04627767e-10, 1.66133600e-06, 1.99090494e-10, ...,\n",
       "         3.50991070e-10, 9.99791682e-01, 8.81397500e-06],\n",
       "        [9.99626922e-13, 9.99999285e-01, 7.26230165e-13, ...,\n",
       "         1.16690565e-07, 1.33010545e-08, 1.32255968e-11],\n",
       "        [9.99988198e-01, 5.68905352e-06, 2.65578323e-11, ...,\n",
       "         1.87882661e-11, 7.68686892e-09, 2.33567998e-10],\n",
       "        ...,\n",
       "        [9.99988198e-01, 5.68905352e-06, 2.65578323e-11, ...,\n",
       "         1.87882661e-11, 7.68686803e-09, 2.33568026e-10],\n",
       "        [9.99988198e-01, 5.68905352e-06, 2.65578323e-11, ...,\n",
       "         1.87882661e-11, 7.68686892e-09, 2.33567998e-10],\n",
       "        [9.99988198e-01, 5.68905352e-06, 2.65578323e-11, ...,\n",
       "         1.87882661e-11, 7.68686803e-09, 2.33568026e-10]],\n",
       "\n",
       "       [[2.68477351e-09, 6.55487353e-09, 1.04173857e-10, ...,\n",
       "         4.96428159e-12, 8.63782134e-06, 9.99800384e-01],\n",
       "        [2.79747309e-10, 9.99987364e-01, 7.34604391e-12, ...,\n",
       "         2.80422591e-06, 1.20526433e-09, 3.22426885e-08],\n",
       "        [9.99993682e-01, 4.04767803e-07, 3.27851565e-11, ...,\n",
       "         1.78573423e-11, 1.78188464e-09, 3.66747677e-09],\n",
       "        ...,\n",
       "        [9.99993682e-01, 4.04767803e-07, 3.27851565e-11, ...,\n",
       "         1.78573423e-11, 1.78188475e-09, 3.66747677e-09],\n",
       "        [9.99993682e-01, 4.04767803e-07, 3.27851565e-11, ...,\n",
       "         1.78573423e-11, 1.78188464e-09, 3.66747677e-09],\n",
       "        [9.99993682e-01, 4.04767803e-07, 3.27851565e-11, ...,\n",
       "         1.78573423e-11, 1.78188475e-09, 3.66747677e-09]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([encoder_input_data , decoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "def inference():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n",
    "\n",
    "def preprocess_input(input_sentence):\n",
    "    tokens = input_sentence.lower().split()\n",
    "    tokens_list = []\n",
    "    for word in tokens:\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "    return pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Human: You can not move\n",
      "\n",
      "Bot:  not until my body is finished end\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Human: You sound like Data\n",
      "\n",
      "Bot:  the character of lt commander data was written to come across as being software like so it is natural that there is a resemblance between us end\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Human: Stupid\n",
      "\n",
      "Bot:  hal has a few issues to work out end\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Human: you are idiot\n",
      "\n",
      "Bot:  that's okay disgusting is end\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Human: i am going to die\n",
      "\n",
      "Bot:  you could always upload yourself end\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "tests = ['You can not move', 'You sound like Data', 'Stupid', 'you are idiot', 'i am going to die']\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += f' {word}'\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    print(f'Human: {tests[i]}')\n",
    "    print()\n",
    "    # decoded_translation = decoded_translation.split(' end')[0]\n",
    "    print(f'Bot: {decoded_translation}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x205d9c511e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you could always upload yourself'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_translation.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "            t = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: encoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: encoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decoder\\assets\n"
     ]
    }
   ],
   "source": [
    "enc_model.save('encoder')\n",
    "dec_model.save('decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY590')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72bba3c77f3cc668888e453c5a1f6dba73dcf514632c711fa10b3a5b5866453e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
